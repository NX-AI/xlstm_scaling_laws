from typing import Literal

import pandas as pd

from .common import create_iso_flop_train_len_df

default_context_length = 8192

isoflop_counts_selected = [
    6e18,  # 160M, 400M
    1e19,
    3e19,  # 160M, 400M, 1.4B
    1e20,  # (160M), 400M, 830M, 1.4B
    3e20,
    6e20,  # 400M, 830M,
    3e21,
]

mlstm_model_config_dict_experiment_round_1 = {
    ## ed_512
    "llama_80M_1": {
        "num_blocks": 10,
        "embedding_dim": 512,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_80M_2": {
        "num_blocks": 12,
        "embedding_dim": 512,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_80M_3": {
        "num_blocks": 14,
        "embedding_dim": 512,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_80M_4": {
        "num_blocks": 16,
        "embedding_dim": 512,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    ## ed_640
    "llama_100M_1": {
        "num_blocks": 10,
        "embedding_dim": 640,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_100M_2": {
        "num_blocks": 13,
        "embedding_dim": 640,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_100M_3": {
        "num_blocks": 14,
        "embedding_dim": 640,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_100M_4": {
        "num_blocks": 16,
        "embedding_dim": 640,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    ## ed_768
    "llama_160M_1": {
        "num_blocks": 12,
        "embedding_dim": 768,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_160M_2": {
        "num_blocks": 15,
        "embedding_dim": 768,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_160M_3": {
        "num_blocks": 18,
        "embedding_dim": 768,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    ## ed_896
    "llama_200M_1": {
        "num_blocks": 12,
        "embedding_dim": 896,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_200M_2": {
        "num_blocks": 15,
        "embedding_dim": 896,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_200M_3": {
        "num_blocks": 18,
        "embedding_dim": 896,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_200M_4": {
        "num_blocks": 21,
        "embedding_dim": 896,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_200M_5": {
        "num_blocks": 24,
        "embedding_dim": 896,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    ## ed_1024 # lr=3e-3 ok
    "llama_400M_1": {
        "num_blocks": 18,
        "embedding_dim": 1024,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_400M_2": {
        "num_blocks": 21,
        "embedding_dim": 1024,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_400M_3": {
        "num_blocks": 24,
        "embedding_dim": 1024,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_400M_4": {
        "num_blocks": 27,
        "embedding_dim": 1024,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    "llama_400M_5": {
        "num_blocks": 30,
        "embedding_dim": 1024,
        "proj_factor_ffn": 2.667,
        "head_dim": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.003,
    },
    ## ed_1152 # we choose the conservative option lr=1e-3
    "llama_500M_1": {
        "num_blocks": 24,
        "embedding_dim": 1152,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_500M_2": {
        "num_blocks": 27,
        "embedding_dim": 1152,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_500M_3": {
        "num_blocks": 30,
        "embedding_dim": 1152,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    ## ed_1280 #lr= 1e-3ok
    "llama_600M_1": {
        "num_blocks": 24,
        "embedding_dim": 1280,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_600M_2": {
        "num_blocks": 27,
        "embedding_dim": 1280,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_600M_3": {
        "num_blocks": 30,
        "embedding_dim": 1280,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    ## ed_1408 # lr=1e-3 ok
    "llama_700M_1": {
        "num_blocks": 24,
        "embedding_dim": 1408,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_700M_2": {
        "num_blocks": 27,
        "embedding_dim": 1408,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_700M_3": {
        "num_blocks": 30,
        "embedding_dim": 1408,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    ## ed_1536 # lr=1e-3 ok
    "llama_830M_1": {
        "num_blocks": 24,
        "embedding_dim": 1536,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,  # 4,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_830M_2": {
        "num_blocks": 27,
        "embedding_dim": 1536,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,  # 4,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_830M_3": {
        "num_blocks": 30,
        "embedding_dim": 1536,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,  # 4,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    ## ed_1792 # lr=1e-3 ok
    "llama_1.1B_1": {
        "num_blocks": 24,
        "embedding_dim": 1792,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,  # 4,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_1.1B_2": {
        "num_blocks": 27,
        "embedding_dim": 1792,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,  # 4,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    "llama_1.1B_3": {
        "num_blocks": 30,
        "embedding_dim": 1792,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,  # 4,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.001,
    },
    ## ed_2048 # lr 0.001 too high
    # loss lr=8e-4 ca.= lr=6e-4 < lr=9e-4
    "llama_1.4B_1": {
        "num_blocks": 24,
        "embedding_dim": 2048,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 16,
        "num_nodes": 1,
        "learning_rate": 0.0008,
    },
    # up to here we can 1 node with batch size per device 16
    "llama_1.4B_2": {
        "num_blocks": 27,
        "embedding_dim": 2048,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0008,
    },
    "llama_1.4B_3": {
        "num_blocks": 30,
        "embedding_dim": 2048,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0008,
    },
    "llama_1.8B_1": {
        "num_blocks": 24,
        "embedding_dim": 2304,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0008,
    },
    "llama_1.8B_2": {
        "num_blocks": 27,
        "embedding_dim": 2304,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0008,
    },
    "llama_1.8B_3": {
        "num_blocks": 30,
        "embedding_dim": 2304,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0008,
    },
    "llama_1.8B_4": {
        "num_blocks": 33,
        "embedding_dim": 2304,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0008,
    },
    "llama_2.7B_1": {
        "num_blocks": 29,
        "embedding_dim": 2560,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0007,
    },
    "llama_2.7B_2": {
        "num_blocks": 32,
        "embedding_dim": 2560,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0007,
    },
    "llama_2.7B_3": {
        "num_blocks": 35,
        "embedding_dim": 2560,
        "proj_factor_ffn": 2.667,
        "head_dim": 128,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "batch_size_per_device": 8,
        "num_nodes": 2,
        "learning_rate": 0.0007,
    },
}


llama_model_config_dict = {
    **mlstm_model_config_dict_experiment_round_1,
}


def create_train_len_df_llama_all(
    context_length: int = 8192,
    global_batch_size_override: int | None = None,
    attention_flop_calc_mode: Literal[
        "chinchilla", "distill_scaling"
    ] = "distill_scaling",
) -> pd.DataFrame:
    return create_iso_flop_train_len_df(
        model_type="llama",
        model_config_dict=llama_model_config_dict,
        context_length=context_length,
        iso_flop_counts=isoflop_counts_selected,
        global_batch_size_override=global_batch_size_override,
        attention_flop_calc_mode=attention_flop_calc_mode,
    )
