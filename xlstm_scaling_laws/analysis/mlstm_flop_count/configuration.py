
mlstm_model_config = {
    "mlstm_160M": {
        "num_blocks": 12,
        "embedding_dim": 768,
        "proj_factor_ffn": 2.667,
        "num_heads": 6,
        "proj_factor_qk": 0.5,
        "chunk_size": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "learning_rate": 0.003,
    },
    "mlstm_400M": {
        "num_blocks": 24,
        "embedding_dim": 1024,
        "proj_factor_ffn": 2.667,
        "num_heads": 4,
        "proj_factor_qk": 0.5,
        "chunk_size": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 128,
        "learning_rate": 0.003,
    },
    "mlstm_830M": {
        "num_blocks": 24,
        "embedding_dim": 1536,
        "proj_factor_ffn": 2.667,
        "num_heads": 6,  # 4,
        "proj_factor_qk": 0.5,
        "chunk_size": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 256,
        "learning_rate": 0.001,
    },
    "mlstm_1.4B": {
        "num_blocks": 24,
        "embedding_dim": 2048,
        "proj_factor_ffn": 2.667,
        "num_heads": 8,
        "proj_factor_qk": 0.5,
        "chunk_size": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 256,
        "learning_rate": 0.0008,
    },
    "mlstm_2.7B": {
        "num_blocks": 32,
        "embedding_dim": 2560,
        "proj_factor_ffn": 2.667,
        "num_heads": 5,
        "proj_factor_qk": 0.5,
        "chunk_size": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 512,
        "learning_rate": 0.0007,
    },
    "mlstm_7B": {
        "num_blocks": 32,
        "embedding_dim": 4096,
        "proj_factor_ffn": 2.667,
        "num_heads": 8,
        "proj_factor_qk": 0.5,
        "chunk_size": 64,
        "vocab_size": 50304,
        "ffn_multiple_of": 64,
        "global_batch_size": 512,
        "learning_rate": 0.0005,
    },
}