{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from xlstm_scaling_laws.analysis.mlstm_flop_count.flop_counting import (\n",
    "    make_mlstm_fw_flop_comparison,\n",
    "    make_mlstm_fwbw_flop_bw_apporox_comparison\n",
    ")\n",
    "from xlstm_scaling_laws.analysis.mlstm_flop_count.configuration import mlstm_model_config\n",
    "\n",
    "import pandas as pd\n",
    "# Set display option for scientific notation\n",
    "pd.options.display.float_format = '{:.2e}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mLSTM Model FLOP analysis\n",
    "\n",
    "Calculate the mLSTM FLOPs with different ways and compare to approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare the two forward FLOP computations for mLSTM\n",
    "\n",
    "\"first\": The first time I implemented the flops counts.\n",
    "\"tfla\": More mature flop counting setup. \n",
    "\n",
    "-> Also check the impact of rounding the ffn dimension to multiple of 64.\n",
    "\n",
    "**Result**: \n",
    "- There is only a slight deviation in the 4th digit. We use tlfa as new default.\n",
    "- We also use rounding of the ffn dim to multiple of 64 the mLSTM FLOPs as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_df = make_mlstm_fw_flop_comparison(mlstm_model_config, include_embedding_flops=False, include_final_logit_flops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df = fw_df.style.format(\n",
    "    \"{:.4e}\",\n",
    "    subset=pd.IndexSlice[\n",
    "        :, fw_df.select_dtypes(include=[\"float64\", \"float32\"]).columns\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dacdc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dacdc_level0_col0\" class=\"col_heading level0 col0\" >model_name</th>\n",
       "      <th id=\"T_dacdc_level0_col1\" class=\"col_heading level0 col1\" >#FLOPs_0_fw-mlstm_fw-first--causal_factor-1.0--round_ffn-False--bw-total_factor_2--incl_emb-False--incl_logit-True</th>\n",
       "      <th id=\"T_dacdc_level0_col2\" class=\"col_heading level0 col2\" >#FLOPs_1_fw-mlstm_fw-tfla--causal_factor-0.75--round_ffn-False--bw-total_factor_2--incl_emb-False--incl_logit-True</th>\n",
       "      <th id=\"T_dacdc_level0_col3\" class=\"col_heading level0 col3\" >#FLOPs_2_fw-mlstm_fw-first--causal_factor-1.0--round_ffn-True--bw-total_factor_2--incl_emb-False--incl_logit-True</th>\n",
       "      <th id=\"T_dacdc_level0_col4\" class=\"col_heading level0 col4\" >#FLOPs_3_fw-mlstm_fw-tfla--causal_factor-1.0--round_ffn-True--bw-total_factor_2--incl_emb-False--incl_logit-True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dacdc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_dacdc_row0_col0\" class=\"data row0 col0\" >mlstm_160M</td>\n",
       "      <td id=\"T_dacdc_row0_col1\" class=\"data row0 col1\" >2.0618e+12</td>\n",
       "      <td id=\"T_dacdc_row0_col2\" class=\"data row0 col2\" >2.0616e+12</td>\n",
       "      <td id=\"T_dacdc_row0_col3\" class=\"data row0 col3\" >2.0906e+12</td>\n",
       "      <td id=\"T_dacdc_row0_col4\" class=\"data row0 col4\" >2.0905e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dacdc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_dacdc_row1_col0\" class=\"data row1 col0\" >mlstm_400M</td>\n",
       "      <td id=\"T_dacdc_row1_col1\" class=\"data row1 col1\" >5.9409e+12</td>\n",
       "      <td id=\"T_dacdc_row1_col2\" class=\"data row1 col2\" >5.9407e+12</td>\n",
       "      <td id=\"T_dacdc_row1_col3\" class=\"data row1 col3\" >5.9663e+12</td>\n",
       "      <td id=\"T_dacdc_row1_col4\" class=\"data row1 col4\" >5.9661e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dacdc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_dacdc_row2_col0\" class=\"data row2 col0\" >mlstm_830M</td>\n",
       "      <td id=\"T_dacdc_row2_col1\" class=\"data row2 col1\" >1.2625e+13</td>\n",
       "      <td id=\"T_dacdc_row2_col2\" class=\"data row2 col2\" >1.2625e+13</td>\n",
       "      <td id=\"T_dacdc_row2_col3\" class=\"data row2 col3\" >1.2740e+13</td>\n",
       "      <td id=\"T_dacdc_row2_col4\" class=\"data row2 col4\" >1.2740e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dacdc_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_dacdc_row3_col0\" class=\"data row3 col0\" >mlstm_1.4B</td>\n",
       "      <td id=\"T_dacdc_row3_col1\" class=\"data row3 col1\" >2.1785e+13</td>\n",
       "      <td id=\"T_dacdc_row3_col2\" class=\"data row3 col2\" >2.1784e+13</td>\n",
       "      <td id=\"T_dacdc_row3_col3\" class=\"data row3 col3\" >2.1886e+13</td>\n",
       "      <td id=\"T_dacdc_row3_col4\" class=\"data row3 col4\" >2.1886e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dacdc_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_dacdc_row4_col0\" class=\"data row4 col0\" >mlstm_2.7B</td>\n",
       "      <td id=\"T_dacdc_row4_col1\" class=\"data row4 col1\" >4.4189e+13</td>\n",
       "      <td id=\"T_dacdc_row4_col2\" class=\"data row4 col2\" >4.4188e+13</td>\n",
       "      <td id=\"T_dacdc_row4_col3\" class=\"data row4 col3\" >4.4271e+13</td>\n",
       "      <td id=\"T_dacdc_row4_col4\" class=\"data row4 col4\" >4.4271e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dacdc_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_dacdc_row5_col0\" class=\"data row5 col0\" >mlstm_7B</td>\n",
       "      <td id=\"T_dacdc_row5_col1\" class=\"data row5 col1\" >1.1030e+14</td>\n",
       "      <td id=\"T_dacdc_row5_col2\" class=\"data row5 col2\" >1.1030e+14</td>\n",
       "      <td id=\"T_dacdc_row5_col3\" class=\"data row5 col3\" >1.1043e+14</td>\n",
       "      <td id=\"T_dacdc_row5_col4\" class=\"data row5 col4\" >1.1043e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7528c954d8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare the Backward FLOP approximations for the mLSTM models\n",
    "\n",
    "Outcome: Factor 2 approximation is the closest to the by hand backward flop computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "count_flops_mlstm_cell_chunkwise_bw__tfla() missing 2 required positional arguments: 'factor_sig' and 'factor_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fwbw_df \u001b[38;5;241m=\u001b[39m \u001b[43mmake_mlstm_fwbw_flop_bw_apporox_comparison\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlstm_model_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_embedding_flops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_final_logit_flops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m fwbw_styled_df \u001b[38;5;241m=\u001b[39m fwbw_df\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:.4e}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     subset\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mIndexSlice[\n\u001b[1;32m      7\u001b[0m         :, fwbw_df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m      8\u001b[0m     ],\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m fwbw_styled_df\n",
      "File \u001b[0;32m~/myrepos/nxai_public/mlstm_scaling_laws/notebooks_analysis/../xlstm_scaling_laws/analysis/mlstm_flop_count/flop_counting.py:214\u001b[0m, in \u001b[0;36mmake_mlstm_fwbw_flop_bw_apporox_comparison\u001b[0;34m(model_config_dict, context_length, include_embedding_flops, include_final_logit_flops)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# add model flop counts fw\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, flop_cfg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(flop_count_cfgs_fw):\n\u001b[1;32m    212\u001b[0m         flop_param_dict[model_key][\n\u001b[1;32m    213\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#FLOPs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fwbw-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflop_cfg\u001b[38;5;241m.\u001b[39mto_config_name(model_type\u001b[38;5;241m=\u001b[39mmodel_type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 214\u001b[0m         ] \u001b[38;5;241m=\u001b[39m \u001b[43mcount_model_flops_fwbw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflop_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    221\u001b[0m df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    222\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame(flop_param_dict)\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/myrepos/nxai_public/mlstm_scaling_laws/notebooks_analysis/../xlstm_scaling_laws/flops/count_flops.py:173\u001b[0m, in \u001b[0;36mcount_model_flops_fwbw\u001b[0;34m(model_type, model_kwargs, context_length, config, num_params)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m seq_mix_flops_fwbw \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sequence mix flops should be 0.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlstm_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     seq_mix_flops_bw_total, seq_mix_flops_bw_recurrent, seq_mix_flops_bw_parallel \u001b[38;5;241m=\u001b[39m \u001b[43mcount_model_flops_bw_mlstm_cell_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     seq_mix_flops_fwbw \u001b[38;5;241m=\u001b[39m seq_mix_flops_bw_total\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/myrepos/nxai_public/mlstm_scaling_laws/notebooks_analysis/../xlstm_scaling_laws/flops/mlstm.py:389\u001b[0m, in \u001b[0;36mcount_model_flops_bw_mlstm_cell_only\u001b[0;34m(model_kwargs, context_length, config)\u001b[0m\n\u001b[1;32m    381\u001b[0m d_v \u001b[38;5;241m=\u001b[39m d_model \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_heads\n\u001b[1;32m    383\u001b[0m num_chunks \u001b[38;5;241m=\u001b[39m context_length \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunk_size\n\u001b[1;32m    385\u001b[0m (\n\u001b[1;32m    386\u001b[0m     total_mlstm_cell_bw_flops,\n\u001b[1;32m    387\u001b[0m     recurrent_mlstm_cell_bw_flops,\n\u001b[1;32m    388\u001b[0m     parallel_mlstm_cell_bw_flops,\n\u001b[0;32m--> 389\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mcount_flops_mlstm_cell_chunkwise_bw__tfla\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_qk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_qk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_v\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlstm_flop_causal_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfactor_exp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflop_factor_exp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfactor_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflop_factor_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m total_flops \u001b[38;5;241m=\u001b[39m total_mlstm_cell_bw_flops \u001b[38;5;241m*\u001b[39m num_blocks\n\u001b[1;32m    401\u001b[0m recurrent_flops \u001b[38;5;241m=\u001b[39m recurrent_mlstm_cell_bw_flops \u001b[38;5;241m*\u001b[39m num_blocks\n",
      "\u001b[0;31mTypeError\u001b[0m: count_flops_mlstm_cell_chunkwise_bw__tfla() missing 2 required positional arguments: 'factor_sig' and 'factor_log'"
     ]
    }
   ],
   "source": [
    "fwbw_df = make_mlstm_fwbw_flop_bw_apporox_comparison(\n",
    "    mlstm_model_config, include_embedding_flops=False, include_final_logit_flops=True\n",
    ")\n",
    "fwbw_styled_df = fwbw_df.style.format(\n",
    "    \"{:.4e}\",\n",
    "    subset=pd.IndexSlice[\n",
    "        :, fwbw_df.select_dtypes(include=[\"float64\", \"float32\"]).columns\n",
    "    ],\n",
    ")\n",
    "fwbw_styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the FLOP approximations from literature\n",
    "\n",
    "We want to show that Kaplan and Distilling scaling law approximations to not apply to the mLSTM.\n",
    "\n",
    "- -> It is a linear RNN and not Transformer with Atttention.\n",
    "- -> We need a different approximation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlstmpt251cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
