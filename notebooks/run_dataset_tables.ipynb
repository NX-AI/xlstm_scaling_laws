{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import logging\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm, Normalize\n",
    "\n",
    "from xlstm_scaling_laws.analysis.parametric_sclaw_fit.data import (\n",
    "    get_all_parametric_sclaw_fit_data_dataframe,\n",
    ")\n",
    "from xlstm_scaling_laws.analysis.parametric_sclaw_fit.plot.plot_model_training_data import (\n",
    "    create_run_data_scatter_plot,\n",
    "    get_combined_run_data_scatter_plot,\n",
    ")\n",
    "from xlstm_scaling_laws.load_data.token_param_ratio import (\n",
    "    create_token_param_ratio_data_table,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(levelname)s: %(message)s\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_row_colors(latex_str):\n",
    "    lines = latex_str.split(\"\\n\")\n",
    "    new_lines = []\n",
    "    in_tabular = False\n",
    "    row_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if \"\\\\begin{tabular}\" in line:\n",
    "            in_tabular = True\n",
    "            new_lines.append(line)\n",
    "        elif \"\\\\end{tabular}\" in line:\n",
    "            in_tabular = False\n",
    "            new_lines.append(line)\n",
    "        elif in_tabular and \"\\\\\\\\\" in line and not line.strip().startswith(\"\\\\\"):\n",
    "            if row_count % 2 == 1:\n",
    "                new_lines.append(\"\\\\rowcolor{gray!10}\" + line)\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "            row_count += 1\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(new_lines)\n",
    "\n",
    "\n",
    "def add_adjustbox_scaling(latex_str, height_scale=0.9):\n",
    "    \"\"\"Add adjustbox scaling to a LaTeX table\"\"\"\n",
    "    lines = latex_str.split(\"\\n\")\n",
    "    new_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"\\\\begin{tabular}\" in line:\n",
    "            new_lines.append(\n",
    "                f\"\\\\begin{{adjustbox}}{{max height={height_scale}\\\\textheight,center}}\"\n",
    "            )\n",
    "            new_lines.append(line)\n",
    "        elif \"\\\\end{tabular}\" in line:\n",
    "            new_lines.append(line)\n",
    "            new_lines.append(\"\\\\end{adjustbox}\")\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(new_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7379502",
   "metadata": {},
   "source": [
    "# Create Run Dataset Model configuration Tables\n",
    "\n",
    "We want to have the following columns in the table:\n",
    "\n",
    "- Parameters (million)\n",
    "- Architecture hyperparams\n",
    "    - embedding dim\n",
    "    - v_head dim\n",
    "    - qk_head dim (only for xLSTM)\n",
    "    - n heads\n",
    "    - ffw dim\n",
    "    - num blocks\n",
    "- Optim parameters\n",
    "    - ctx length\n",
    "    - global batch size\n",
    "    - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550b27a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_set_ctx_length</th>\n",
       "      <th>name</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>model_type</th>\n",
       "      <th>num_params</th>\n",
       "      <th>num_tokens_training</th>\n",
       "      <th>num_flops_training</th>\n",
       "      <th>val/.dclm_loss</th>\n",
       "      <th>token_param_ratio</th>\n",
       "      <th>width_depth_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>proj_factor_ffn</th>\n",
       "      <th>ffn_multiple_of</th>\n",
       "      <th>ffn_dim</th>\n",
       "      <th>head_dim_qk</th>\n",
       "      <th>head_dim_v</th>\n",
       "      <th>IsoFLOP</th>\n",
       "      <th>train/.loss_mean</th>\n",
       "      <th>run_id</th>\n",
       "      <th>model_checkpoint_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps3500_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>3.670016e+09</td>\n",
       "      <td>4.416455e+18</td>\n",
       "      <td>3.298485</td>\n",
       "      <td>22.623585</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.300660</td>\n",
       "      <td>o2y7xnfn</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps5000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>5.242880e+09</td>\n",
       "      <td>6.309221e+18</td>\n",
       "      <td>3.220001</td>\n",
       "      <td>32.319407</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.217189</td>\n",
       "      <td>8b27x4t0</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps7000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>7.340032e+09</td>\n",
       "      <td>8.832909e+18</td>\n",
       "      <td>3.162786</td>\n",
       "      <td>45.247169</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.161473</td>\n",
       "      <td>4ce0r9qc</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps8000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>8.388608e+09</td>\n",
       "      <td>1.009475e+19</td>\n",
       "      <td>3.143749</td>\n",
       "      <td>51.711051</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.142752</td>\n",
       "      <td>x987at37</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps18000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>1.887437e+10</td>\n",
       "      <td>2.271320e+19</td>\n",
       "      <td>3.050733</td>\n",
       "      <td>116.349864</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.051504</td>\n",
       "      <td>1mmzna50</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps36000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>3.774874e+10</td>\n",
       "      <td>4.542639e+19</td>\n",
       "      <td>2.995063</td>\n",
       "      <td>232.699728</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.995211</td>\n",
       "      <td>p2nsobw9</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps87000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>9.122611e+10</td>\n",
       "      <td>1.097804e+20</td>\n",
       "      <td>2.946427</td>\n",
       "      <td>562.357675</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.946148</td>\n",
       "      <td>prhdeg55</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.001_steps173000_gb...</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>1.814036e+11</td>\n",
       "      <td>2.182990e+20</td>\n",
       "      <td>2.933174</td>\n",
       "      <td>1118.251470</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.934087</td>\n",
       "      <td>6i73refq</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps10000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>1.048576e+10</td>\n",
       "      <td>3.525725e+19</td>\n",
       "      <td>2.961880</td>\n",
       "      <td>25.786631</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.966652</td>\n",
       "      <td>22m4wtpw</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps18000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>1.887437e+10</td>\n",
       "      <td>6.346304e+19</td>\n",
       "      <td>2.852509</td>\n",
       "      <td>46.415935</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.853140</td>\n",
       "      <td>299iz8at</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps46000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>4.823450e+10</td>\n",
       "      <td>1.621833e+20</td>\n",
       "      <td>2.760690</td>\n",
       "      <td>118.618501</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.761025</td>\n",
       "      <td>dag13m13</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps87000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>9.122611e+10</td>\n",
       "      <td>3.067380e+20</td>\n",
       "      <td>2.718793</td>\n",
       "      <td>224.343687</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.718626</td>\n",
       "      <td>viush2xo</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.001_steps427000_gb...</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>4.477420e+11</td>\n",
       "      <td>1.505484e+21</td>\n",
       "      <td>2.648728</td>\n",
       "      <td>1101.089133</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.649103</td>\n",
       "      <td>710fum8k-l8ergzdk</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.001_steps215000_gb...</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>2.254438e+11</td>\n",
       "      <td>7.580308e+20</td>\n",
       "      <td>2.675238</td>\n",
       "      <td>554.412561</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.675270</td>\n",
       "      <td>5fn8fajw</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps10000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>2.097152e+10</td>\n",
       "      <td>1.337267e+20</td>\n",
       "      <td>2.778400</td>\n",
       "      <td>25.143103</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.779873</td>\n",
       "      <td>7ms1oq9z</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps18000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>3.774874e+10</td>\n",
       "      <td>2.407081e+20</td>\n",
       "      <td>2.698914</td>\n",
       "      <td>45.257585</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.698153</td>\n",
       "      <td>auk1f8wb</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps46000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>9.646899e+10</td>\n",
       "      <td>6.151429e+20</td>\n",
       "      <td>2.603439</td>\n",
       "      <td>115.658272</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.603839</td>\n",
       "      <td>hzg6axyz</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps90000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>1.887437e+11</td>\n",
       "      <td>1.203541e+21</td>\n",
       "      <td>2.551973</td>\n",
       "      <td>226.287924</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.552181</td>\n",
       "      <td>5z8xqrtc</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps220000_gb...</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>4.613734e+11</td>\n",
       "      <td>2.941988e+21</td>\n",
       "      <td>2.501723</td>\n",
       "      <td>553.148259</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.504537</td>\n",
       "      <td>el7dzild</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps16000_gb...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>3.355443e+10</td>\n",
       "      <td>3.470852e+20</td>\n",
       "      <td>2.634025</td>\n",
       "      <td>23.623285</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.633174</td>\n",
       "      <td>gqbsl7we</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps31000_gb...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>6.501171e+10</td>\n",
       "      <td>6.724777e+20</td>\n",
       "      <td>2.562962</td>\n",
       "      <td>45.770114</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.562756</td>\n",
       "      <td>trg7ko91</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps76000_gb...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>1.593836e+11</td>\n",
       "      <td>1.648655e+21</td>\n",
       "      <td>2.480129</td>\n",
       "      <td>112.210603</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.477411</td>\n",
       "      <td>dgfaq11l</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps150000_g...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>3.145728e+11</td>\n",
       "      <td>3.253924e+21</td>\n",
       "      <td>2.436433</td>\n",
       "      <td>221.468294</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.434990</td>\n",
       "      <td>ihtb18ge</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps375000_g...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>7.864320e+11</td>\n",
       "      <td>8.134810e+21</td>\n",
       "      <td>2.390363</td>\n",
       "      <td>553.670736</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.390698</td>\n",
       "      <td>1udj4i0u</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps16000_gb...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>6.710886e+10</td>\n",
       "      <td>1.341800e+21</td>\n",
       "      <td>2.485858</td>\n",
       "      <td>24.143803</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.488384</td>\n",
       "      <td>tsonwoey</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps31000_gb...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>1.300234e+11</td>\n",
       "      <td>2.599738e+21</td>\n",
       "      <td>2.407212</td>\n",
       "      <td>46.778619</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.408017</td>\n",
       "      <td>qmbhsvn9</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps76000_gb...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>3.187671e+11</td>\n",
       "      <td>6.373551e+21</td>\n",
       "      <td>2.336316</td>\n",
       "      <td>114.683066</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.337427</td>\n",
       "      <td>yb3g4d8r</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps146000_g...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>6.123684e+11</td>\n",
       "      <td>1.224393e+22</td>\n",
       "      <td>2.294343</td>\n",
       "      <td>220.312205</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.293955</td>\n",
       "      <td>0d5qdik1</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_7B_ctx8192_lr0.0005_steps76000_gbs256</td>\n",
       "      <td>scl_llama_7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>6.863196e+09</td>\n",
       "      <td>1.593836e+11</td>\n",
       "      <td>7.403171e+21</td>\n",
       "      <td>2.278404</td>\n",
       "      <td>23.222934</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>10944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.276851</td>\n",
       "      <td>2ha0dhyb</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs256</td>\n",
       "      <td>scl_llama_7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>6.863196e+09</td>\n",
       "      <td>3.040870e+11</td>\n",
       "      <td>1.412447e+22</td>\n",
       "      <td>2.224290</td>\n",
       "      <td>44.306914</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>10944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.227857</td>\n",
       "      <td>bpjiwg80</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs512</td>\n",
       "      <td>scl_llama_7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>6.863196e+09</td>\n",
       "      <td>6.081741e+11</td>\n",
       "      <td>2.824894e+22</td>\n",
       "      <td>2.181154</td>\n",
       "      <td>88.613827</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>10944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.181396</td>\n",
       "      <td>m76cguk9</td>\n",
       "      <td>[\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_set_ctx_length  \\\n",
       "609        tokenparam_ctx8192   \n",
       "610        tokenparam_ctx8192   \n",
       "611        tokenparam_ctx8192   \n",
       "612        tokenparam_ctx8192   \n",
       "613        tokenparam_ctx8192   \n",
       "614        tokenparam_ctx8192   \n",
       "615        tokenparam_ctx8192   \n",
       "616        tokenparam_ctx8192   \n",
       "617        tokenparam_ctx8192   \n",
       "618        tokenparam_ctx8192   \n",
       "619        tokenparam_ctx8192   \n",
       "620        tokenparam_ctx8192   \n",
       "621        tokenparam_ctx8192   \n",
       "622        tokenparam_ctx8192   \n",
       "623        tokenparam_ctx8192   \n",
       "624        tokenparam_ctx8192   \n",
       "625        tokenparam_ctx8192   \n",
       "626        tokenparam_ctx8192   \n",
       "627        tokenparam_ctx8192   \n",
       "628        tokenparam_ctx8192   \n",
       "629        tokenparam_ctx8192   \n",
       "630        tokenparam_ctx8192   \n",
       "631        tokenparam_ctx8192   \n",
       "632        tokenparam_ctx8192   \n",
       "633        tokenparam_ctx8192   \n",
       "634        tokenparam_ctx8192   \n",
       "635        tokenparam_ctx8192   \n",
       "636        tokenparam_ctx8192   \n",
       "637        tokenparam_ctx8192   \n",
       "638        tokenparam_ctx8192   \n",
       "639        tokenparam_ctx8192   \n",
       "\n",
       "                                                  name           run_tag  \\\n",
       "609   dclm_llama_160M_ctx8192_lr0.003_steps3500_gbs128    scl_llama_160M   \n",
       "610   dclm_llama_160M_ctx8192_lr0.003_steps5000_gbs128    scl_llama_160M   \n",
       "611   dclm_llama_160M_ctx8192_lr0.003_steps7000_gbs128    scl_llama_160M   \n",
       "612   dclm_llama_160M_ctx8192_lr0.003_steps8000_gbs128    scl_llama_160M   \n",
       "613  dclm_llama_160M_ctx8192_lr0.003_steps18000_gbs128    scl_llama_160M   \n",
       "614  dclm_llama_160M_ctx8192_lr0.003_steps36000_gbs128    scl_llama_160M   \n",
       "615  dclm_llama_160M_ctx8192_lr0.003_steps87000_gbs128    scl_llama_160M   \n",
       "616  dclm_llama_160M_ctx8192_lr0.001_steps173000_gb...    scl_llama_160M   \n",
       "617  dclm_llama_400M_ctx8192_lr0.003_steps10000_gbs128    scl_llama_400M   \n",
       "618  dclm_llama_400M_ctx8192_lr0.003_steps18000_gbs128    scl_llama_400M   \n",
       "619  dclm_llama_400M_ctx8192_lr0.003_steps46000_gbs128    scl_llama_400M   \n",
       "620  dclm_llama_400M_ctx8192_lr0.003_steps87000_gbs128    scl_llama_400M   \n",
       "621  dclm_llama_400M_ctx8192_lr0.001_steps427000_gb...    scl_llama_400M   \n",
       "622  dclm_llama_400M_ctx8192_lr0.001_steps215000_gb...    scl_llama_400M   \n",
       "623  dclm_llama_830M_ctx8192_lr0.001_steps10000_gbs256  scl_llama_830Mv2   \n",
       "624  dclm_llama_830M_ctx8192_lr0.001_steps18000_gbs256  scl_llama_830Mv2   \n",
       "625  dclm_llama_830M_ctx8192_lr0.001_steps46000_gbs256  scl_llama_830Mv2   \n",
       "626  dclm_llama_830M_ctx8192_lr0.001_steps90000_gbs256  scl_llama_830Mv2   \n",
       "627  dclm_llama_830M_ctx8192_lr0.001_steps220000_gb...  scl_llama_830Mv2   \n",
       "628  dclm_llama_1.4B_ctx8192_lr0.0008_steps16000_gb...  scl_llama_1.4Bv2   \n",
       "629  dclm_llama_1.4B_ctx8192_lr0.0008_steps31000_gb...  scl_llama_1.4Bv2   \n",
       "630  dclm_llama_1.4B_ctx8192_lr0.0008_steps76000_gb...  scl_llama_1.4Bv2   \n",
       "631  dclm_llama_1.4B_ctx8192_lr0.0008_steps150000_g...  scl_llama_1.4Bv2   \n",
       "632  dclm_llama_1.4B_ctx8192_lr0.0008_steps375000_g...  scl_llama_1.4Bv2   \n",
       "633  dclm_llama_2.7B_ctx8192_lr0.0007_steps16000_gb...    scl_llama_2.7B   \n",
       "634  dclm_llama_2.7B_ctx8192_lr0.0007_steps31000_gb...    scl_llama_2.7B   \n",
       "635  dclm_llama_2.7B_ctx8192_lr0.0007_steps76000_gb...    scl_llama_2.7B   \n",
       "636  dclm_llama_2.7B_ctx8192_lr0.0007_steps146000_g...    scl_llama_2.7B   \n",
       "637   dclm_llama_7B_ctx8192_lr0.0005_steps76000_gbs256      scl_llama_7B   \n",
       "638  dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs256      scl_llama_7B   \n",
       "639  dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs512      scl_llama_7B   \n",
       "\n",
       "    model_type    num_params  num_tokens_training  num_flops_training  \\\n",
       "609      llama  1.622208e+08         3.670016e+09        4.416455e+18   \n",
       "610      llama  1.622208e+08         5.242880e+09        6.309221e+18   \n",
       "611      llama  1.622208e+08         7.340032e+09        8.832909e+18   \n",
       "612      llama  1.622208e+08         8.388608e+09        1.009475e+19   \n",
       "613      llama  1.622208e+08         1.887437e+10        2.271320e+19   \n",
       "614      llama  1.622208e+08         3.774874e+10        4.542639e+19   \n",
       "615      llama  1.622208e+08         9.122611e+10        1.097804e+20   \n",
       "616      llama  1.622208e+08         1.814036e+11        2.182990e+20   \n",
       "617      llama  4.066355e+08         1.048576e+10        3.525725e+19   \n",
       "618      llama  4.066355e+08         1.887437e+10        6.346304e+19   \n",
       "619      llama  4.066355e+08         4.823450e+10        1.621833e+20   \n",
       "620      llama  4.066355e+08         9.122611e+10        3.067380e+20   \n",
       "621      llama  4.066355e+08         4.477420e+11        1.505484e+21   \n",
       "622      llama  4.066355e+08         2.254438e+11        7.580308e+20   \n",
       "623      llama  8.340864e+08         2.097152e+10        1.337267e+20   \n",
       "624      llama  8.340864e+08         3.774874e+10        2.407081e+20   \n",
       "625      llama  8.340864e+08         9.646899e+10        6.151429e+20   \n",
       "626      llama  8.340864e+08         1.887437e+11        1.203541e+21   \n",
       "627      llama  8.340864e+08         4.613734e+11        2.941988e+21   \n",
       "628      llama  1.420397e+09         3.355443e+10        3.470852e+20   \n",
       "629      llama  1.420397e+09         6.501171e+10        6.724777e+20   \n",
       "630      llama  1.420397e+09         1.593836e+11        1.648655e+21   \n",
       "631      llama  1.420397e+09         3.145728e+11        3.253924e+21   \n",
       "632      llama  1.420397e+09         7.864320e+11        8.134810e+21   \n",
       "633      llama  2.779548e+09         6.710886e+10        1.341800e+21   \n",
       "634      llama  2.779548e+09         1.300234e+11        2.599738e+21   \n",
       "635      llama  2.779548e+09         3.187671e+11        6.373551e+21   \n",
       "636      llama  2.779548e+09         6.123684e+11        1.224393e+22   \n",
       "637      llama  6.863196e+09         1.593836e+11        7.403171e+21   \n",
       "638      llama  6.863196e+09         3.040870e+11        1.412447e+22   \n",
       "639      llama  6.863196e+09         6.081741e+11        2.824894e+22   \n",
       "\n",
       "     val/.dclm_loss  token_param_ratio  width_depth_ratio  ... num_heads  \\\n",
       "609        3.298485          22.623585          64.000000  ...        12   \n",
       "610        3.220001          32.319407          64.000000  ...        12   \n",
       "611        3.162786          45.247169          64.000000  ...        12   \n",
       "612        3.143749          51.711051          64.000000  ...        12   \n",
       "613        3.050733         116.349864          64.000000  ...        12   \n",
       "614        2.995063         232.699728          64.000000  ...        12   \n",
       "615        2.946427         562.357675          64.000000  ...        12   \n",
       "616        2.933174        1118.251470          64.000000  ...        12   \n",
       "617        2.961880          25.786631          42.666667  ...        16   \n",
       "618        2.852509          46.415935          42.666667  ...        16   \n",
       "619        2.760690         118.618501          42.666667  ...        16   \n",
       "620        2.718793         224.343687          42.666667  ...        16   \n",
       "621        2.648728        1101.089133          42.666667  ...        16   \n",
       "622        2.675238         554.412561          42.666667  ...        16   \n",
       "623        2.778400          25.143103          64.000000  ...        16   \n",
       "624        2.698914          45.257585          64.000000  ...        16   \n",
       "625        2.603439         115.658272          64.000000  ...        16   \n",
       "626        2.551973         226.287924          64.000000  ...        16   \n",
       "627        2.501723         553.148259          64.000000  ...        16   \n",
       "628        2.634025          23.623285          85.333333  ...        16   \n",
       "629        2.562962          45.770114          85.333333  ...        16   \n",
       "630        2.480129         112.210603          85.333333  ...        16   \n",
       "631        2.436433         221.468294          85.333333  ...        16   \n",
       "632        2.390363         553.670736          85.333333  ...        16   \n",
       "633        2.485858          24.143803          80.000000  ...        32   \n",
       "634        2.407212          46.778619          80.000000  ...        32   \n",
       "635        2.336316         114.683066          80.000000  ...        32   \n",
       "636        2.294343         220.312205          80.000000  ...        32   \n",
       "637        2.278404          23.222934         128.000000  ...        32   \n",
       "638        2.224290          44.306914         128.000000  ...        32   \n",
       "639        2.181154          88.613827         128.000000  ...        32   \n",
       "\n",
       "    proj_factor_ffn  ffn_multiple_of  ffn_dim  head_dim_qk  head_dim_v  \\\n",
       "609           2.667               64     2048          NaN          64   \n",
       "610           2.667               64     2048          NaN          64   \n",
       "611           2.667               64     2048          NaN          64   \n",
       "612           2.667               64     2048          NaN          64   \n",
       "613           2.667               64     2048          NaN          64   \n",
       "614           2.667               64     2048          NaN          64   \n",
       "615           2.667               64     2048          NaN          64   \n",
       "616           2.667               64     2048          NaN          64   \n",
       "617           2.667               64     2752          NaN          64   \n",
       "618           2.667               64     2752          NaN          64   \n",
       "619           2.667               64     2752          NaN          64   \n",
       "620           2.667               64     2752          NaN          64   \n",
       "621           2.667               64     2752          NaN          64   \n",
       "622           2.667               64     2752          NaN          64   \n",
       "623           2.667               64     4096          NaN          96   \n",
       "624           2.667               64     4096          NaN          96   \n",
       "625           2.667               64     4096          NaN          96   \n",
       "626           2.667               64     4096          NaN          96   \n",
       "627           2.667               64     4096          NaN          96   \n",
       "628           2.667               64     5504          NaN         128   \n",
       "629           2.667               64     5504          NaN         128   \n",
       "630           2.667               64     5504          NaN         128   \n",
       "631           2.667               64     5504          NaN         128   \n",
       "632           2.667               64     5504          NaN         128   \n",
       "633           2.667               64     6848          NaN          80   \n",
       "634           2.667               64     6848          NaN          80   \n",
       "635           2.667               64     6848          NaN          80   \n",
       "636           2.667               64     6848          NaN          80   \n",
       "637           2.667               64    10944          NaN         128   \n",
       "638           2.667               64    10944          NaN         128   \n",
       "639           2.667               64    10944          NaN         128   \n",
       "\n",
       "     IsoFLOP train/.loss_mean             run_id  \\\n",
       "609      NaN         3.300660           o2y7xnfn   \n",
       "610      NaN         3.217189           8b27x4t0   \n",
       "611      NaN         3.161473           4ce0r9qc   \n",
       "612      NaN         3.142752           x987at37   \n",
       "613      NaN         3.051504           1mmzna50   \n",
       "614      NaN         2.995211           p2nsobw9   \n",
       "615      NaN         2.946148           prhdeg55   \n",
       "616      NaN         2.934087           6i73refq   \n",
       "617      NaN         2.966652           22m4wtpw   \n",
       "618      NaN         2.853140           299iz8at   \n",
       "619      NaN         2.761025           dag13m13   \n",
       "620      NaN         2.718626           viush2xo   \n",
       "621      NaN         2.649103  710fum8k-l8ergzdk   \n",
       "622      NaN         2.675270           5fn8fajw   \n",
       "623      NaN         2.779873           7ms1oq9z   \n",
       "624      NaN         2.698153           auk1f8wb   \n",
       "625      NaN         2.603839           hzg6axyz   \n",
       "626      NaN         2.552181           5z8xqrtc   \n",
       "627      NaN         2.504537           el7dzild   \n",
       "628      NaN         2.633174           gqbsl7we   \n",
       "629      NaN         2.562756           trg7ko91   \n",
       "630      NaN         2.477411           dgfaq11l   \n",
       "631      NaN         2.434990           ihtb18ge   \n",
       "632      NaN         2.390698           1udj4i0u   \n",
       "633      NaN         2.488384           tsonwoey   \n",
       "634      NaN         2.408017           qmbhsvn9   \n",
       "635      NaN         2.337427           yb3g4d8r   \n",
       "636      NaN         2.293955           0d5qdik1   \n",
       "637      NaN         2.276851           2ha0dhyb   \n",
       "638      NaN         2.227857           bpjiwg80   \n",
       "639      NaN         2.181396           m76cguk9   \n",
       "\n",
       "                                model_checkpoint_paths  \n",
       "609  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "610  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "611  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "612  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "613  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "614  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "615  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "616  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "617  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "618  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "619  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "620  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "621  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "622  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "623  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "624  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "625  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "626  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "627  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "628  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "629  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "630  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "631  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "632  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "633  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "634  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "635  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "636  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "637  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "638  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "639  [\"/nfs-gpu/xlstm/outputs_beck/sclaw/dclm_llama...  \n",
       "\n",
       "[31 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "df[(df[\"experiment_set\"] == \"tokenparam\") & (df[\"model_type\"] == \"llama\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcc26bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1347372475364666e+23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6.400000e+02\n",
       "mean     4.898027e+20\n",
       "std      3.935773e+21\n",
       "min      2.809833e+18\n",
       "25%      9.981188e+18\n",
       "50%      2.992133e+19\n",
       "75%      3.064443e+19\n",
       "max      8.480968e+22\n",
       "Name: num_flops_training, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get min FLOPs, max FLOPs, min train tokens, max train tokens, min params, max params\n",
    "df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "print(df[\"num_flops_training\"].sum())\n",
    "df[\"num_flops_training\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f48ae94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.400000e+02\n",
       "mean     3.852149e+10\n",
       "std      1.374194e+11\n",
       "min      1.887437e+09\n",
       "25%      4.404019e+09\n",
       "50%      8.808038e+09\n",
       "75%      2.013266e+10\n",
       "max      2.097152e+12\n",
       "Name: num_tokens_training, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"num_tokens_training\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e7da50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.400000e+02\n",
       "mean     7.374013e+08\n",
       "std      1.050585e+09\n",
       "min      8.363469e+07\n",
       "25%      2.046973e+08\n",
       "50%      4.066355e+08\n",
       "75%      8.340864e+08\n",
       "max      6.867523e+09\n",
       "Name: num_params, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"num_params\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf4132",
   "metadata": {},
   "source": [
    "## Model Configuration Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1404e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_set_df(exp_set: str | list[str], model_type: str) -> pd.DataFrame:\n",
    "    mlstm_df = get_all_parametric_sclaw_fit_data_dataframe(model_type=model_type)\n",
    "    if model_type == \"mlstm\":\n",
    "        sel_cols = [\n",
    "            \"num_params\",\n",
    "            \"embedding_dim\",\n",
    "            \"ffn_dim\",\n",
    "            \"head_dim_qk\",\n",
    "            \"head_dim_v\",\n",
    "            \"num_heads\",\n",
    "            \"num_blocks\",\n",
    "            # \"context_length\",\n",
    "            # \"global_batch_size\",\n",
    "            # \"learning_rate\",\n",
    "        ]\n",
    "    elif model_type == \"llama\":\n",
    "        sel_cols = [\n",
    "            \"num_params\",\n",
    "            \"embedding_dim\",\n",
    "            \"ffn_dim\",\n",
    "            \"head_dim_v\",\n",
    "            \"num_heads\",\n",
    "            \"num_blocks\",\n",
    "            # \"context_length\",\n",
    "            # \"global_batch_size\",\n",
    "            # \"learning_rate\",\n",
    "        ]\n",
    "    if \"tokenparam\" in exp_set:\n",
    "        sel_cols += [\"global_batch_size\", \"learning_rate\"]\n",
    "    if isinstance(exp_set, str):\n",
    "        exp_set = [exp_set]\n",
    "    exp_set_df = (\n",
    "        mlstm_df[mlstm_df[\"experiment_set_ctx_length\"].isin(exp_set)][sel_cols]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(by=[\"num_params\"])\n",
    "    )\n",
    "    if \"head_dim_qk\" in sel_cols:\n",
    "        exp_set_df[\"head_dim_qk\"] = exp_set_df[\"head_dim_qk\"].astype(int)\n",
    "\n",
    "    if \"global_batch_size\" in sel_cols:\n",
    "        exp_set_df[\"global_batch_size\"] = exp_set_df[\"global_batch_size\"].astype(int)\n",
    "\n",
    "    # convert num_params in millions\n",
    "    exp_set_df[\"num_params\"] = (exp_set_df[\"num_params\"] / 1e6).astype(int)\n",
    "    exp_set_df = exp_set_df.rename(columns={\"num_params\": \"num_params (M)\"})\n",
    "    exp_set_df = exp_set_df.reset_index(drop=True)\n",
    "\n",
    "    # add a \\ before each _ in column names for latex\n",
    "    # exp_set_df.columns = [col.replace(\"_\", \"\\\\_\") for col in exp_set_df.columns]\n",
    "\n",
    "    # prettify column names\n",
    "    if model_type == \"mlstm\":\n",
    "        col_name_map = {\n",
    "            \"num_params (M)\": \"\\#Params (M)\",\n",
    "            \"embedding_dim\": r\"$d_{\\text{model}}$\",\n",
    "            \"ffn_dim\": r\"$d_{\\text{ff}}$\",\n",
    "            \"head_dim_qk\": r\"$d_{\\text{qk}}$\",\n",
    "            \"head_dim_v\": r\"$d_{\\text{hv}}$\",\n",
    "            \"num_heads\": r\"$n_{\\text{heads}}$\",\n",
    "            \"num_blocks\": r\"$n_{\\text{layer}}$\",\n",
    "            \"context_length\": r\"$T$ (ctx)\",\n",
    "            \"global_batch_size\": r\"$B$ (batch)\",\n",
    "            \"learning_rate\": \"LR\",\n",
    "        }\n",
    "    elif model_type == \"llama\":\n",
    "        col_name_map = {\n",
    "            \"num_params (M)\": \"\\#Params (M)\",\n",
    "            \"embedding_dim\": r\"$d_{\\text{model}}$\",\n",
    "            \"ffn_dim\": r\"$d_{\\text{ff}}$\",\n",
    "            \"head_dim_v\": r\"$d_{\\text{v}}$\",\n",
    "            \"num_heads\": r\"$n_{\\text{heads}}$\",\n",
    "            \"num_blocks\": r\"$n_{\\text{layer}}$\",\n",
    "            \"context_length\": r\"$T$ (ctx)\",\n",
    "            \"global_batch_size\": r\"$B$ (batch)\",\n",
    "            \"learning_rate\": \"LR\",\n",
    "        }\n",
    "    exp_set_df = exp_set_df.rename(columns=col_name_map)\n",
    "\n",
    "    return exp_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaee9b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for xLSTM models trained with the Token/Param configuration.}\n",
      "\\label{tab:tokenparam_hyperparams}\n",
      "\\begin{tabular}{r|rrrrrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{qk}}$ & $d_{\\text{hv}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ & $B$ (batch) & LR \\\\\n",
      "\\midrule\n",
      "164 & 768 & 2112 & 64 & 128 & 6 & 12 & 128 & 3e-3 \\\\\n",
      "\\rowcolor{gray!10}406 & 1024 & 2752 & 128 & 256 & 4 & 24 & 128 & 3e-3 \\\\\n",
      "406 & 1024 & 2752 & 128 & 256 & 4 & 24 & 128 & 1e-3 \\\\\n",
      "\\rowcolor{gray!10}841 & 1536 & 4160 & 192 & 384 & 4 & 24 & 256 & 1e-3 \\\\\n",
      "841 & 1536 & 4160 & 192 & 384 & 4 & 24 & 256 & 8e-4 \\\\\n",
      "\\rowcolor{gray!10}1420 & 2048 & 5504 & 256 & 512 & 4 & 24 & 256 & 8e-4 \\\\\n",
      "1420 & 2048 & 5504 & 256 & 512 & 4 & 24 & 256 & 7e-4 \\\\\n",
      "\\rowcolor{gray!10}2780 & 2560 & 6848 & 256 & 512 & 5 & 32 & 512 & 7e-4 \\\\\n",
      "6865 & 4096 & 10944 & 256 & 512 & 8 & 32 & 512 & 5e-4 \\\\\n",
      "\\rowcolor{gray!10}6865 & 4096 & 10944 & 256 & 512 & 8 & 32 & 256 & 5e-4 \\\\\n",
      "6865 & 4096 & 10944 & 256 & 512 & 8 & 32 & 512 & 4e-4 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mlstm token param table\n",
    "df = get_experiment_set_df(\"tokenparam_ctx8192\", \"mlstm\")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for xLSTM models trained with the Token/Param configuration.\",\n",
    "    label=\"tab:tokenparam_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2056bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for Transformer models trained with the Token/Param configuration.}\n",
      "\\label{tab:tokenparam_hyperparams}\n",
      "\\begin{tabular}{r|rrrrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{v}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ & $B$ (batch) & LR \\\\\n",
      "\\midrule\n",
      "162 & 768 & 2048 & 64 & 12 & 12 & 128 & 3e-3 \\\\\n",
      "\\rowcolor{gray!10}162 & 768 & 2048 & 64 & 12 & 12 & 128 & 1e-3 \\\\\n",
      "406 & 1024 & 2752 & 64 & 16 & 24 & 128 & 3e-3 \\\\\n",
      "\\rowcolor{gray!10}406 & 1024 & 2752 & 64 & 16 & 24 & 128 & 1e-3 \\\\\n",
      "834 & 1536 & 4096 & 96 & 16 & 24 & 256 & 1e-3 \\\\\n",
      "\\rowcolor{gray!10}1420 & 2048 & 5504 & 128 & 16 & 24 & 256 & 8e-4 \\\\\n",
      "2779 & 2560 & 6848 & 80 & 32 & 32 & 512 & 7e-4 \\\\\n",
      "\\rowcolor{gray!10}6863 & 4096 & 10944 & 128 & 32 & 32 & 256 & 5e-4 \\\\\n",
      "6863 & 4096 & 10944 & 128 & 32 & 32 & 512 & 5e-4 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama token param table\n",
    "df = get_experiment_set_df(\"tokenparam_ctx8192\", \"llama\")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for Transformer models trained with the Token/Param configuration.\",\n",
    "    label=\"tab:tokenparam_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0464e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for xLSTM models trained with the IsoFLOP configuration.}\n",
      "\\label{tab:xlstm_isoflop_hyperparams}\n",
      "\\begin{adjustbox}{max height=0.5\\textheight,center}\n",
      "\\begin{tabular}{r|rrrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{qk}}$ & $d_{\\text{hv}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ \\\\\n",
      "\\midrule\n",
      "83 & 512 & 1408 & 64 & 128 & 4 & 10 \\\\\n",
      "\\rowcolor{gray!10}90 & 512 & 1408 & 64 & 128 & 4 & 12 \\\\\n",
      "96 & 512 & 1408 & 64 & 128 & 4 & 14 \\\\\n",
      "\\rowcolor{gray!10}102 & 512 & 1408 & 64 & 128 & 4 & 16 \\\\\n",
      "114 & 640 & 1728 & 64 & 128 & 5 & 10 \\\\\n",
      "\\rowcolor{gray!10}123 & 640 & 1728 & 64 & 128 & 5 & 12 \\\\\n",
      "128 & 640 & 1728 & 64 & 128 & 5 & 13 \\\\\n",
      "\\rowcolor{gray!10}133 & 640 & 1728 & 64 & 128 & 5 & 14 \\\\\n",
      "143 & 640 & 1728 & 64 & 128 & 5 & 16 \\\\\n",
      "\\rowcolor{gray!10}164 & 768 & 2112 & 64 & 128 & 6 & 12 \\\\\n",
      "185 & 768 & 2112 & 64 & 128 & 6 & 15 \\\\\n",
      "\\rowcolor{gray!10}207 & 896 & 2432 & 64 & 128 & 7 & 12 \\\\\n",
      "207 & 768 & 2112 & 64 & 128 & 6 & 18 \\\\\n",
      "\\rowcolor{gray!10}236 & 896 & 2432 & 64 & 128 & 7 & 15 \\\\\n",
      "265 & 896 & 2432 & 64 & 128 & 7 & 18 \\\\\n",
      "\\rowcolor{gray!10}295 & 896 & 2432 & 64 & 128 & 7 & 21 \\\\\n",
      "324 & 896 & 2432 & 64 & 128 & 7 & 24 \\\\\n",
      "\\rowcolor{gray!10}330 & 1024 & 2752 & 128 & 256 & 4 & 18 \\\\\n",
      "353 & 896 & 2432 & 64 & 128 & 7 & 27 \\\\\n",
      "\\rowcolor{gray!10}368 & 1024 & 2752 & 128 & 256 & 4 & 21 \\\\\n",
      "406 & 1024 & 2752 & 128 & 256 & 4 & 24 \\\\\n",
      "\\rowcolor{gray!10}444 & 1024 & 2752 & 128 & 256 & 4 & 27 \\\\\n",
      "482 & 1024 & 2752 & 128 & 256 & 4 & 30 \\\\\n",
      "\\rowcolor{gray!10}503 & 1152 & 3136 & 64 & 128 & 9 & 24 \\\\\n",
      "552 & 1152 & 3136 & 64 & 128 & 9 & 27 \\\\\n",
      "\\rowcolor{gray!10}601 & 1152 & 3136 & 64 & 128 & 9 & 30 \\\\\n",
      "604 & 1280 & 3456 & 128 & 256 & 5 & 24 \\\\\n",
      "\\rowcolor{gray!10}664 & 1280 & 3456 & 128 & 256 & 5 & 27 \\\\\n",
      "715 & 1408 & 3776 & 64 & 128 & 11 & 24 \\\\\n",
      "\\rowcolor{gray!10}724 & 1280 & 3456 & 128 & 256 & 5 & 30 \\\\\n",
      "787 & 1408 & 3776 & 64 & 128 & 11 & 27 \\\\\n",
      "\\rowcolor{gray!10}841 & 1536 & 4160 & 128 & 256 & 6 & 24 \\\\\n",
      "859 & 1408 & 3776 & 64 & 128 & 11 & 30 \\\\\n",
      "\\rowcolor{gray!10}927 & 1536 & 4160 & 128 & 256 & 6 & 27 \\\\\n",
      "1013 & 1536 & 4160 & 128 & 256 & 6 & 30 \\\\\n",
      "\\rowcolor{gray!10}1108 & 1792 & 4800 & 128 & 256 & 7 & 24 \\\\\n",
      "1224 & 1792 & 4800 & 128 & 256 & 7 & 27 \\\\\n",
      "\\rowcolor{gray!10}1340 & 1792 & 4800 & 128 & 256 & 7 & 30 \\\\\n",
      "1421 & 2048 & 5504 & 128 & 256 & 8 & 24 \\\\\n",
      "\\rowcolor{gray!10}1573 & 2048 & 5504 & 128 & 256 & 8 & 27 \\\\\n",
      "1772 & 2304 & 6208 & 128 & 256 & 9 & 24 \\\\\n",
      "\\rowcolor{gray!10}1876 & 2048 & 5504 & 128 & 256 & 8 & 33 \\\\\n",
      "1964 & 2304 & 6208 & 128 & 256 & 9 & 27 \\\\\n",
      "\\rowcolor{gray!10}2028 & 2048 & 5504 & 128 & 256 & 8 & 36 \\\\\n",
      "2157 & 2304 & 6208 & 128 & 256 & 9 & 30 \\\\\n",
      "\\rowcolor{gray!10}2350 & 2304 & 6208 & 128 & 256 & 9 & 33 \\\\\n",
      "2781 & 2560 & 6848 & 128 & 256 & 10 & 32 \\\\\n",
      "\\rowcolor{gray!10}3017 & 2560 & 6848 & 128 & 256 & 10 & 35 \\\\\n",
      "3150 & 2816 & 7552 & 128 & 256 & 11 & 30 \\\\\n",
      "\\rowcolor{gray!10}3254 & 2560 & 6848 & 128 & 256 & 10 & 38 \\\\\n",
      "3342 & 2816 & 7552 & 128 & 256 & 11 & 32 \\\\\n",
      "\\rowcolor{gray!10}3533 & 2816 & 7552 & 128 & 256 & 11 & 34 \\\\\n",
      "3724 & 2816 & 7552 & 128 & 256 & 11 & 36 \\\\\n",
      "\\rowcolor{gray!10}3726 & 3072 & 8256 & 128 & 256 & 12 & 30 \\\\\n",
      "3954 & 3072 & 8256 & 128 & 256 & 12 & 32 \\\\\n",
      "\\rowcolor{gray!10}4410 & 3072 & 8256 & 128 & 256 & 12 & 36 \\\\\n",
      "4597 & 3328 & 8896 & 128 & 256 & 13 & 32 \\\\\n",
      "\\rowcolor{gray!10}5130 & 3328 & 8896 & 128 & 256 & 13 & 36 \\\\\n",
      "5311 & 3584 & 9600 & 128 & 256 & 14 & 32 \\\\\n",
      "\\rowcolor{gray!10}5930 & 3584 & 9600 & 128 & 256 & 14 & 36 \\\\\n",
      "6464 & 4096 & 10944 & 128 & 256 & 16 & 30 \\\\\n",
      "\\rowcolor{gray!10}6867 & 4096 & 10944 & 128 & 256 & 16 & 32 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mlstm isoflop table\n",
    "df = get_experiment_set_df(\n",
    "    [\"isoflop_ctx2048\", \"isoflop_ctx8192\", \"isoflop_ctx16384\"], \"mlstm\"\n",
    ")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for xLSTM models trained with the IsoFLOP configuration.\",\n",
    "    label=\"tab:xlstm_isoflop_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "colored_latex_table = add_adjustbox_scaling(colored_latex_table, height_scale=0.5)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40b3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for Transformer models trained with the IsoFLOP configuration.}\n",
      "\\label{tab:transformer_isoflop_hyperparams}\n",
      "\\begin{adjustbox}{max height=0.5\\textheight,center}\n",
      "\\begin{tabular}{r|rrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{v}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ \\\\\n",
      "\\midrule\n",
      "83 & 512 & 1408 & 64 & 8 & 10 \\\\\n",
      "\\rowcolor{gray!10}90 & 512 & 1408 & 64 & 8 & 12 \\\\\n",
      "96 & 512 & 1408 & 64 & 8 & 14 \\\\\n",
      "\\rowcolor{gray!10}102 & 512 & 1408 & 64 & 8 & 16 \\\\\n",
      "113 & 640 & 1728 & 64 & 10 & 10 \\\\\n",
      "\\rowcolor{gray!10}128 & 640 & 1728 & 64 & 10 & 13 \\\\\n",
      "133 & 640 & 1728 & 64 & 10 & 14 \\\\\n",
      "\\rowcolor{gray!10}143 & 640 & 1728 & 64 & 10 & 16 \\\\\n",
      "162 & 768 & 2048 & 64 & 12 & 12 \\\\\n",
      "\\rowcolor{gray!10}183 & 768 & 2048 & 64 & 12 & 15 \\\\\n",
      "204 & 768 & 2048 & 64 & 12 & 18 \\\\\n",
      "\\rowcolor{gray!10}207 & 896 & 2432 & 64 & 14 & 12 \\\\\n",
      "236 & 896 & 2432 & 64 & 14 & 15 \\\\\n",
      "\\rowcolor{gray!10}265 & 896 & 2432 & 64 & 14 & 18 \\\\\n",
      "294 & 896 & 2432 & 64 & 14 & 21 \\\\\n",
      "\\rowcolor{gray!10}324 & 896 & 2432 & 64 & 14 & 24 \\\\\n",
      "330 & 1024 & 2752 & 64 & 16 & 18 \\\\\n",
      "\\rowcolor{gray!10}368 & 1024 & 2752 & 64 & 16 & 21 \\\\\n",
      "406 & 1024 & 2752 & 64 & 16 & 24 \\\\\n",
      "\\rowcolor{gray!10}444 & 1024 & 2752 & 64 & 16 & 27 \\\\\n",
      "482 & 1024 & 2752 & 64 & 16 & 30 \\\\\n",
      "\\rowcolor{gray!10}498 & 1152 & 3072 & 128 & 9 & 24 \\\\\n",
      "545 & 1152 & 3072 & 128 & 9 & 27 \\\\\n",
      "\\rowcolor{gray!10}593 & 1152 & 3072 & 128 & 9 & 30 \\\\\n",
      "604 & 1280 & 3456 & 128 & 10 & 24 \\\\\n",
      "\\rowcolor{gray!10}664 & 1280 & 3456 & 128 & 10 & 27 \\\\\n",
      "714 & 1408 & 3776 & 128 & 11 & 24 \\\\\n",
      "\\rowcolor{gray!10}723 & 1280 & 3456 & 128 & 10 & 30 \\\\\n",
      "786 & 1408 & 3776 & 128 & 11 & 27 \\\\\n",
      "\\rowcolor{gray!10}834 & 1536 & 4096 & 128 & 12 & 24 \\\\\n",
      "858 & 1408 & 3776 & 128 & 11 & 30 \\\\\n",
      "\\rowcolor{gray!10}919 & 1536 & 4096 & 128 & 12 & 27 \\\\\n",
      "1003 & 1536 & 4096 & 128 & 12 & 30 \\\\\n",
      "\\rowcolor{gray!10}1107 & 1792 & 4800 & 128 & 14 & 24 \\\\\n",
      "1223 & 1792 & 4800 & 128 & 14 & 27 \\\\\n",
      "\\rowcolor{gray!10}1339 & 1792 & 4800 & 128 & 14 & 30 \\\\\n",
      "1420 & 2048 & 5504 & 128 & 16 & 24 \\\\\n",
      "\\rowcolor{gray!10}1572 & 2048 & 5504 & 128 & 16 & 27 \\\\\n",
      "1723 & 2048 & 5504 & 128 & 16 & 30 \\\\\n",
      "\\rowcolor{gray!10}1760 & 2304 & 6144 & 128 & 18 & 24 \\\\\n",
      "1951 & 2304 & 6144 & 128 & 18 & 27 \\\\\n",
      "\\rowcolor{gray!10}2142 & 2304 & 6144 & 128 & 18 & 30 \\\\\n",
      "2334 & 2304 & 6144 & 128 & 18 & 33 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama isoflop table\n",
    "df = get_experiment_set_df(\n",
    "    [\"isoflop_ctx2048\", \"isoflop_ctx8192\", \"isoflop_ctx16384\"], \"llama\"\n",
    ")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for Transformer models trained with the IsoFLOP configuration.\",\n",
    "    label=\"tab:transformer_isoflop_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "colored_latex_table = add_adjustbox_scaling(colored_latex_table, height_scale=0.5)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f6cb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_set_ctx_length</th>\n",
       "      <th>name</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>num_params</th>\n",
       "      <th>ffn_dim</th>\n",
       "      <th>head_dim_qk</th>\n",
       "      <th>head_dim_v</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>context_length</th>\n",
       "      <th>train/.loss_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>isoflop_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb3...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.464058e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>128.0</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.553666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>isoflop_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb3...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.867523e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>128.0</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.562390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps73000_gb...</td>\n",
       "      <td>scl_mlstm_7B</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.206036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps76000_gb...</td>\n",
       "      <td>scl_mlstm_7B</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.251832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps181000_g...</td>\n",
       "      <td>scl_mlstm_7B</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.150207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_gbs512</td>\n",
       "      <td>dclm_mLSTMv1_7B_longrun_pretraining_final</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.100448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_set_ctx_length  \\\n",
       "228           isoflop_ctx8192   \n",
       "229           isoflop_ctx8192   \n",
       "605        tokenparam_ctx8192   \n",
       "606        tokenparam_ctx8192   \n",
       "607        tokenparam_ctx8192   \n",
       "608        tokenparam_ctx8192   \n",
       "\n",
       "                                                  name  \\\n",
       "228  dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb3...   \n",
       "229  dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb3...   \n",
       "605  dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps73000_gb...   \n",
       "606  dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps76000_gb...   \n",
       "607  dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps181000_g...   \n",
       "608                     dclm_mLSTMv1_7B_ctx8192_gbs512   \n",
       "\n",
       "                                       run_tag  learning_rate  \\\n",
       "228                           sclaw_iso_round8         0.0009   \n",
       "229                           sclaw_iso_round8         0.0009   \n",
       "605                               scl_mlstm_7B         0.0005   \n",
       "606                               scl_mlstm_7B         0.0005   \n",
       "607                               scl_mlstm_7B         0.0005   \n",
       "608  dclm_mLSTMv1_7B_longrun_pretraining_final         0.0004   \n",
       "\n",
       "     global_batch_size    num_params  ffn_dim  head_dim_qk  head_dim_v  \\\n",
       "228              256.0  6.464058e+09    10944        128.0         256   \n",
       "229              256.0  6.867523e+09    10944        128.0         256   \n",
       "605              512.0  6.865425e+09    10944        256.0         512   \n",
       "606              256.0  6.865425e+09    10944        256.0         512   \n",
       "607              512.0  6.865425e+09    10944        256.0         512   \n",
       "608              512.0  6.865425e+09    10944        256.0         512   \n",
       "\n",
       "     num_heads  num_blocks  context_length  train/.loss_mean  \n",
       "228         16          30            8192          2.553666  \n",
       "229         16          32            8192          2.562390  \n",
       "605          8          32            8192          2.206036  \n",
       "606          8          32            8192          2.251832  \n",
       "607          8          32            8192          2.150207  \n",
       "608          8          32            8192          2.100448  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlstm_df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"mlstm\")\n",
    "mlstm_df[mlstm_df[\"embedding_dim\"] == 4096][\n",
    "    [\n",
    "        \"experiment_set_ctx_length\",\n",
    "        \"name\",\n",
    "        \"run_tag\",\n",
    "        \"learning_rate\",\n",
    "        \"global_batch_size\",\n",
    "        \"num_params\",\n",
    "        \"ffn_dim\",\n",
    "        \"head_dim_qk\",\n",
    "        \"head_dim_v\",\n",
    "        \"num_heads\",\n",
    "        \"num_blocks\",\n",
    "        \"context_length\",\n",
    "        \"train/.loss_mean\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4891ba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1707200/1683853249.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df[df[\"experiment_set_ctx_length\"] == \"isoflop_ctx8192\"][\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>IsoFLOP</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>dclm_mLSTMv1_100M_ctx8192_lr0.003_steps192000_...</td>\n",
       "      <td>nb10_ed640_nh5_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>dclm_mLSTMv1_160M_ctx8192_lr0.003_steps126500_...</td>\n",
       "      <td>nb12_ed768_nh6_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>dclm_mLSTMv1_200M_ctx8192_lr0.003_steps97500_n...</td>\n",
       "      <td>nb12_ed896_nh7_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>dclm_mLSTMv1_100M_ctx8192_lr0.003_steps162000_...</td>\n",
       "      <td>nb13_ed640_nh5_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>dclm_mLSTMv1_200M_ctx8192_lr0.003_steps82500_n...</td>\n",
       "      <td>nb15_ed896_nh7_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>dclm_mLSTMv1_4.5B_ctx8192_lr0.0009_steps10600_...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb3...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb3...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps8200_n...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>dclm_mLSTMv1_3.7B_ctx8192_lr0.0009_steps11200_...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name  \\\n",
       "90   dclm_mLSTMv1_100M_ctx8192_lr0.003_steps192000_...   \n",
       "91   dclm_mLSTMv1_160M_ctx8192_lr0.003_steps126500_...   \n",
       "96   dclm_mLSTMv1_200M_ctx8192_lr0.003_steps97500_n...   \n",
       "101  dclm_mLSTMv1_100M_ctx8192_lr0.003_steps162000_...   \n",
       "108  dclm_mLSTMv1_200M_ctx8192_lr0.003_steps82500_n...   \n",
       "..                                                 ...   \n",
       "227  dclm_mLSTMv1_4.5B_ctx8192_lr0.0009_steps10600_...   \n",
       "228  dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb3...   \n",
       "229  dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb3...   \n",
       "230  dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps8200_n...   \n",
       "231  dclm_mLSTMv1_3.7B_ctx8192_lr0.0009_steps11200_...   \n",
       "\n",
       "                              run_tag  global_batch_size IsoFLOP  \\\n",
       "90   nb10_ed640_nh5_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "91   nb12_ed768_nh6_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "96   nb12_ed896_nh7_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "101  nb13_ed640_nh5_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "108  nb15_ed896_nh7_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "..                                ...                ...     ...   \n",
       "227                  sclaw_iso_round8              256.0   6e+20   \n",
       "228                  sclaw_iso_round8              256.0   6e+20   \n",
       "229                  sclaw_iso_round8              256.0   6e+20   \n",
       "230                  sclaw_iso_round8              256.0   6e+20   \n",
       "231                  sclaw_iso_round8              256.0   6e+20   \n",
       "\n",
       "     learning_rate  \n",
       "90          0.0030  \n",
       "91          0.0030  \n",
       "96          0.0030  \n",
       "101         0.0030  \n",
       "108         0.0030  \n",
       "..             ...  \n",
       "227         0.0009  \n",
       "228         0.0009  \n",
       "229         0.0009  \n",
       "230         0.0009  \n",
       "231         0.0009  \n",
       "\n",
       "[132 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "df[df[\"experiment_set_ctx_length\"] == \"isoflop_ctx8192\"][\n",
    "    df[\"IsoFLOP\"].isin([\"6e+18\", \"6e+20\", \"1e+20\"])\n",
    "][\n",
    "    [\n",
    "        \"name\",\n",
    "        \"run_tag\",\n",
    "        \"global_batch_size\",\n",
    "        \"IsoFLOP\",\n",
    "        \"learning_rate\",\n",
    "    ]\n",
    "].sort_values(by=[\"IsoFLOP\", \"global_batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f333f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isoflop_batch_size_df(ctx_length=None) -> pd.DataFrame:\n",
    "    df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "    batch_size_df = (\n",
    "        df[[\"IsoFLOP\", \"context_length\", \"global_batch_size\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(by=[\"context_length\", \"IsoFLOP\", \"global_batch_size\"])\n",
    "        .dropna()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    batch_size_df.rename(columns={\"global_batch_size\": \"bs_in_seqs\"}, inplace=True)\n",
    "    batch_size_df[\"bs_in_seqs\"] = batch_size_df[\"bs_in_seqs\"].astype(int)\n",
    "\n",
    "    batch_size_df[\"bs_in_tokens\"] = (\n",
    "        batch_size_df[\"bs_in_seqs\"] * batch_size_df[\"context_length\"]\n",
    "    )\n",
    "    batch_size_df[\"bs_in_tokens\"] = batch_size_df[\"bs_in_tokens\"].astype(int)\n",
    "\n",
    "    # rename columns for latex\n",
    "    col_name_map = {\n",
    "        \"IsoFLOP\": \"IsoFLOP\",\n",
    "        \"context_length\": r\"$T$ (ctx)\",\n",
    "        \"bs_in_seqs\": r\"$B$ (seqs)\",\n",
    "        \"bs_in_tokens\": r\"$B \\times T$ (tokens)\",\n",
    "    }\n",
    "    batch_size_df = batch_size_df.rename(columns=col_name_map)\n",
    "    if ctx_length is not None:\n",
    "        return batch_size_df[batch_size_df[r\"$T$ (ctx)\"] == ctx_length]\n",
    "    return batch_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409fe5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Batch sizes used for models trained with the IsoFLOP configuration at context length 8192.}\n",
      "\\label{tab:isoflop_batch_sizes}\n",
      "\\begin{tabular}{r|rrr}\n",
      "\\toprule\n",
      "IsoFLOP & $T$ (ctx) & $B$ (seqs) & $B \\times T$ (tokens) \\\\\n",
      "\\midrule\n",
      "\\rowcolor{gray!10}1e+19 & 8192 & 128 & 1,048,576 \\\\\n",
      "1e+20 & 8192 & 128 & 1,048,576 \\\\\n",
      "\\rowcolor{gray!10}3e+19 & 8192 & 128 & 1,048,576 \\\\\n",
      "6e+18 & 8192 & 128 & 1,048,576 \\\\\n",
      "\\rowcolor{gray!10}6e+20 & 8192 & 256 & 2,097,152 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_isoflop_batch_size_df(8192)\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={r\"$B \\times T$ (tokens)\": lambda x: f\"{x:,}\"},\n",
    "    caption=\"Batch sizes used for models trained with the IsoFLOP configuration at context length 8192.\",\n",
    "    label=\"tab:isoflop_batch_sizes\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "# colored_latex_table = add_adjustbox_scaling(colored_latex_table, height_scale=0.5)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ecf742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533039c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_260210_pt291cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
