{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import logging\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap, LogNorm, Normalize\n",
    "\n",
    "from xlstm_scaling_laws.analysis.parametric_sclaw_fit.data import (\n",
    "    get_all_parametric_sclaw_fit_data_dataframe,\n",
    ")\n",
    "from xlstm_scaling_laws.analysis.parametric_sclaw_fit.plot.plot_model_training_data import (\n",
    "    create_run_data_scatter_plot,\n",
    "    get_combined_run_data_scatter_plot,\n",
    ")\n",
    "from xlstm_scaling_laws.load_data.token_param_ratio import (\n",
    "    create_token_param_ratio_data_table,\n",
    ")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(levelname)s: %(message)s\",\n",
    "    force=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_row_colors(latex_str):\n",
    "    lines = latex_str.split(\"\\n\")\n",
    "    new_lines = []\n",
    "    in_tabular = False\n",
    "    row_count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if \"\\\\begin{tabular}\" in line:\n",
    "            in_tabular = True\n",
    "            new_lines.append(line)\n",
    "        elif \"\\\\end{tabular}\" in line:\n",
    "            in_tabular = False\n",
    "            new_lines.append(line)\n",
    "        elif in_tabular and \"\\\\\\\\\" in line and not line.strip().startswith(\"\\\\\"):\n",
    "            if row_count % 2 == 1:\n",
    "                new_lines.append(\"\\\\rowcolor{gray!10}\" + line)\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "            row_count += 1\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(new_lines)\n",
    "\n",
    "\n",
    "def add_adjustbox_scaling(latex_str, height_scale=0.9):\n",
    "    \"\"\"Add adjustbox scaling to a LaTeX table\"\"\"\n",
    "    lines = latex_str.split(\"\\n\")\n",
    "    new_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"\\\\begin{tabular}\" in line:\n",
    "            new_lines.append(\n",
    "                f\"\\\\begin{{adjustbox}}{{max height={height_scale}\\\\textheight,center}}\"\n",
    "            )\n",
    "            new_lines.append(line)\n",
    "        elif \"\\\\end{tabular}\" in line:\n",
    "            new_lines.append(line)\n",
    "            new_lines.append(\"\\\\end{adjustbox}\")\n",
    "        else:\n",
    "            new_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(new_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7379502",
   "metadata": {},
   "source": [
    "# Create Run Dataset Model configuration Tables\n",
    "\n",
    "We want to have the following columns in the table:\n",
    "\n",
    "- Parameters (million)\n",
    "- Architecture hyperparams\n",
    "    - embedding dim\n",
    "    - v_head dim\n",
    "    - qk_head dim (only for xLSTM)\n",
    "    - n heads\n",
    "    - ffw dim\n",
    "    - num blocks\n",
    "- Optim parameters\n",
    "    - ctx length\n",
    "    - global batch size\n",
    "    - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550b27a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512, id=0m5bmumq, path=['xlstm', 'xlstm_jax', '0m5bmumq'], created_at=2025-04-13T06:17:04Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512, id=ch1anxyv, path=['xlstm', 'xlstm_jax', 'ch1anxyv'], created_at=2025-04-13T17:35:13Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512, id=52pij92y, path=['xlstm', 'xlstm_jax', '52pij92y'], created_at=2025-04-13T08:12:40Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512, id=oyn1hwkt, path=['xlstm', 'xlstm_jax', 'oyn1hwkt'], created_at=2025-04-14T17:41:02Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512, id=94x65c87, path=['xlstm', 'xlstm_jax', '94x65c87'], created_at=2025-04-14T21:52:45Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512, id=1dylnc0q, path=['xlstm', 'xlstm_jax', '1dylnc0q'], created_at=2025-04-14T03:16:17Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512, id=zhigonop, path=['xlstm', 'xlstm_jax', 'zhigonop'], created_at=2025-04-14T12:27:31Z)\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps16400_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_200M_ctx2048_lr0.003_steps3000_nb27_ed896_nh7_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_80M_ctx2048_lr0.003_steps27200_nb10_ed512_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2200_nb33_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2200_nb33_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2200_nb33_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2200_nb33_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2200_nb33_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2200_nb33_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2400_nb30_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2400_nb30_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2400_nb30_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2400_nb30_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2400_nb30_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx2048_lr0.002_steps2400_nb30_ed2304_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_500M_ctx2048_lr0.002_steps9600_nb27_ed1152_nh9_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_700M_ctx2048_lr0.002_steps7400_nb24_ed1408_nh11_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.4B_ctx2048_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_400M_ctx2048_lr0.003_steps14800_nb21_ed1024_nh4_pf2.667_gbs512'. Returning NaN.\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256, id=5erwjaxl, path=['xlstm', 'xlstm_jax', '5erwjaxl'], created_at=2025-03-30T09:17:49Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256, id=adot2rjx, path=['xlstm', 'xlstm_jax', 'adot2rjx'], created_at=2025-03-22T08:10:09Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256, id=jk24amzl, path=['xlstm', 'xlstm_jax', 'jk24amzl'], created_at=2025-03-22T08:10:08Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256, id=i0l9fwds, path=['xlstm', 'xlstm_jax', 'i0l9fwds'], created_at=2025-03-29T11:32:18Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256, id=5kmoduar, path=['xlstm', 'xlstm_jax', '5kmoduar'], created_at=2025-02-19T06:41:45Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256, id=0mk7sak7, path=['xlstm', 'xlstm_jax', '0mk7sak7'], created_at=2025-04-04T16:39:18Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512, id=a7eh9h1e, path=['xlstm', 'xlstm_jax', 'a7eh9h1e'], created_at=2025-03-12T10:22:11Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128, id=2c0iqntm, path=['xlstm', 'xlstm_jax', '2c0iqntm'], created_at=2025-02-17T01:04:19Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128, id=j9egd83d, path=['xlstm', 'xlstm_jax', 'j9egd83d'], created_at=2025-02-17T08:15:20Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128, id=yssq2ha9, path=['xlstm', 'xlstm_jax', 'yssq2ha9'], created_at=2025-02-17T14:07:25Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256, id=elmmvqbd, path=['xlstm', 'xlstm_jax', 'elmmvqbd'], created_at=2025-03-29T21:35:36Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256, id=6hds9u81, path=['xlstm', 'xlstm_jax', '6hds9u81'], created_at=2025-02-24T13:56:34Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256, id=0mk7sak7, path=['xlstm', 'xlstm_jax', '0mk7sak7'], created_at=2025-04-04T16:39:18Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256, id=yelxn36u, path=['xlstm', 'xlstm_jax', 'yelxn36u'], created_at=2025-04-04T00:58:03Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256, id=a3n0nsep, path=['xlstm', 'xlstm_jax', 'a3n0nsep'], created_at=2025-04-08T10:45:49Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256, id=om1w6tr3, path=['xlstm', 'xlstm_jax', 'om1w6tr3'], created_at=2025-04-07T03:29:19Z)\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_830M_ctx8192_lr0.002_steps10400_nb24_ed1536_nh6_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.001_steps2400_nb24_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps2000_nb27_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.1B_ctx8192_lr0.0009_steps1800_nb30_ed1792_nh7_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.0007_steps9000_nb32_ed2560_nh10_pf2.667_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps24500_nb15_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps4000_nb21_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_200M_ctx8192_lr0.003_steps56500_nb24_ed896_nh7_pf2.667_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps4000_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps4000_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps4000_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps4000_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps4000_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps4000_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps24500_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps24500_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps24500_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps24500_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps24500_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.4B_ctx8192_lr0.0008_steps24500_nb36_ed2048_nh8_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps4800_nb24_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3750_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3750_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3750_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3750_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3750_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3750_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps116000_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps116000_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps116000_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps116000_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps116000_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps116000_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps23250_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps23250_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps23250_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps23250_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps23250_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps23250_nb30_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps106000_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps106000_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps106000_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps106000_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps106000_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps106000_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps21250_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps21250_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps21250_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps21250_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps21250_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps21250_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx8192_lr0.0008_steps3500_nb33_ed2304_nh9_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.7B_ctx8192_lr0.001_steps11200_nb36_ed3072_nh12_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.7B_ctx8192_lr0.001_steps11200_nb36_ed3072_nh12_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.7B_ctx8192_lr0.001_steps11200_nb36_ed3072_nh12_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.7B_ctx8192_lr0.001_steps11200_nb36_ed3072_nh12_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.7B_ctx8192_lr0.001_steps11200_nb36_ed3072_nh12_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_3.7B_ctx8192_lr0.001_steps11200_nb36_ed3072_nh12_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_2.7B_ctx8192_lr0.001_steps17800_nb32_ed2560_nh10_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_3.1B_ctx8192_lr0.001_steps14000_nb34_ed2816_nh11_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_nb32_ed3584_nh14_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256'. Returning NaN.\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64, id=o75fh34j, path=['xlstm', 'xlstm_jax', 'o75fh34j'], created_at=2025-04-18T05:51:55Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64, id=wufh9cz7, path=['xlstm', 'xlstm_jax', 'wufh9cz7'], created_at=2025-04-15T18:48:55Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64, id=ajl1sky2, path=['xlstm', 'xlstm_jax', 'ajl1sky2'], created_at=2025-04-19T16:49:51Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64, id=boaq806o, path=['xlstm', 'xlstm_jax', 'boaq806o'], created_at=2025-04-19T18:53:13Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64, id=ybyfmcvv, path=['xlstm', 'xlstm_jax', 'ybyfmcvv'], created_at=2025-04-17T11:34:49Z)\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.0008_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.0008_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.0008_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.0008_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.0008_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.0008_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2400_nb30_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2400_nb30_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2400_nb30_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2400_nb30_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2400_nb30_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2400_nb30_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.001_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.001_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.001_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.001_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.001_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_2.7B_ctx16384_lr0.001_steps2000_nb29_ed2560_nh10_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps2400_nb36_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.8B_ctx16384_lr0.001_steps2200_nb33_ed2304_nh9_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_1.4B_ctx16384_lr0.002_steps3600_nb24_ed2048_nh8_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps4400_nb24_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_400M_ctx16384_lr0.003_steps3600_nb30_ed1024_nh4_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_mLSTMv1_160M_ctx16384_lr0.003_steps5600_nb18_ed768_nh6_pf2.667_gbs64'. Returning NaN.\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512, id=008q2hk5, path=['xlstm', 'xlstm_jax', '008q2hk5'], created_at=2025-04-11T20:27:01Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512, id=bcfaw1es, path=['xlstm', 'xlstm_jax', 'bcfaw1es'], created_at=2025-04-12T00:40:28Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512, id=xk2woz8u, path=['xlstm', 'xlstm_jax', 'xk2woz8u'], created_at=2025-04-12T05:04:02Z)\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_80M_ctx2048_lr0.003_steps11200_nb14_ed512_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_200M_ctx2048_lr0.003_steps3000_nb24_ed896_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_100M_ctx2048_lr0.003_steps14000_nb13_ed640_hd64_gbs512'. Returning NaN.\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128, id=qnz6n90e, path=['xlstm', 'xlstm_jax', 'qnz6n90e'], created_at=2025-04-10T11:00:28Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128, id=ck44j8mo, path=['xlstm', 'xlstm_jax', 'ck44j8mo'], created_at=2025-04-22T15:38:30Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128, id=dh31jij9, path=['xlstm', 'xlstm_jax', 'dh31jij9'], created_at=2025-04-20T04:23:12Z)\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_80M_ctx8192_lr0.003_steps12200_nb14_ed512_hd64_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_1.1B_ctx8192_lr0.0008_steps10400_nb27_ed1792_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_1.8B_ctx8192_lr0.0008_steps7600_nb24_ed2304_hd128_gbs128'. Returning NaN.\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64, id=91qf3tp8, path=['xlstm', 'xlstm_jax', '91qf3tp8'], created_at=2025-04-19T06:33:31Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64, id=v57ztygq, path=['xlstm', 'xlstm_jax', 'v57ztygq'], created_at=2025-04-16T04:33:00Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64, id=nerl7kls, path=['xlstm', 'xlstm_jax', 'nerl7kls'], created_at=2025-04-16T17:13:31Z)\n",
      "KeyError: 'dataset/'. While creating RunData from WandBRunData(name=dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64, id=nuy7tmkt, path=['xlstm', 'xlstm_jax', 'nuy7tmkt'], created_at=2025-04-17T04:29:18Z)\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_400M_ctx16384_lr0.003_steps5600_nb27_ed1024_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_1.4B_ctx16384_lr0.0008_steps2200_nb24_ed2048_hd128_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_160M_ctx16384_lr0.003_steps5800_nb12_ed768_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.step_time' not found in run 'dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64'. Returning NaN.\n",
      "Log key 'train/.loss_mean' not found in run 'dclm_llama_200M_ctx16384_lr0.003_steps2000_nb18_ed896_hd64_gbs64'. Returning NaN.\n",
      "Could not extract chunk size from backend kwargs '{'backend_name': 'chunkwise--triton_xl_chunk'}' from run WandBRunData(name=dclm_mLSTMv1_160M_ctx8192_lr0.003_steps18000_gbs128, id=djkqz1dc, path=['xlstm', 'xlstm_jax', 'djkqz1dc'], created_at=2024-12-19T15:53:36Z).Using default chunk size of 64.\n",
      "Could not extract chunk size from backend kwargs '{'backend_name': 'max_triton_noslice'}' from run WandBRunData(name=dclm_mLSTMv1_7B_ctx8192_gbs512, id=hphy3hsq, path=['xlstm', 'xlstm_jax', 'hphy3hsq'], created_at=2024-11-06T22:21:03Z).Using default chunk size of 64.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiment_set_ctx_length",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "run_tag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_params",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_tokens_training",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_flops_training",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val/.dclm_loss",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "token_param_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "width_depth_ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Preset Token Param Ratio",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "experiment_set",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "context_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "global_batch_size",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_train_steps",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val/.dclm_perplexity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Preset Num Params",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Model Size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "embedding_dim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_blocks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_heads",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "proj_factor_ffn",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ffn_multiple_of",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ffn_dim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "head_dim_qk",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "head_dim_v",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "IsoFLOP",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "train/.loss_mean",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "db7699d5-9d0c-4892-b2cd-94c148e05115",
       "rows": [
        [
         "641",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps3500_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "3670016000.0",
         "4.416454740934656e+18",
         "3.2984854356248934",
         "22.623584645125657",
         "64.0",
         "22",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "3500.0",
         "27.071606164852376",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "3.3006604026314403"
        ],
        [
         "642",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps5000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "5242880000.0",
         "6.30922105847808e+18",
         "3.220000824745079",
         "32.31940663589379",
         "64.0",
         "extra",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "5000.0",
         "25.02814082316528",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "3.2171888468561147"
        ],
        [
         "643",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps7000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "7340032000.0",
         "8.832909481869312e+18",
         "3.1627858629517873",
         "45.247169290251314",
         "64.0",
         "44",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "7000.0",
         "23.636351930094158",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "3.161473189028275"
        ],
        [
         "644",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps8000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "8388608000.0",
         "1.0094753693564928e+19",
         "3.14374949543583",
         "51.71105061743007",
         "64.0",
         "44",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "8000.0",
         "23.190657310580374",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "3.14275226358362"
        ],
        [
         "645",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps18000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "18874368000.0",
         "2.271319581052109e+19",
         "3.0507329804048577",
         "116.34986388921766",
         "64.0",
         "110",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "18000.0",
         "21.130827229848148",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "3.0515043240379107"
        ],
        [
         "646",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps36000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "37748736000.0",
         "4.542639162104218e+19",
         "2.9950632150868985",
         "232.69972777843532",
         "64.0",
         "220",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "36000.0",
         "19.98662330605232",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "2.995210521363668"
        ],
        [
         "647",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.003_steps87000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "91226112000.0",
         "1.097804464175186e+20",
         "2.9464265550680695",
         "562.357675464552",
         "64.0",
         "550",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "87000.0",
         "19.03780149635812",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "2.946148295695177"
        ],
        [
         "648",
         "tokenparam_ctx8192",
         "dclm_llama_160M_ctx8192_lr0.001_steps173000_gbs128",
         "scl_llama_160M",
         "llama",
         "162220800.0",
         "181403648000.0",
         "2.1829904862334157e+20",
         "2.9331741101918065",
         "1118.2514696019252",
         "64.0",
         "1100",
         "tokenparam",
         "8192",
         "0.001",
         "128.0",
         "173000.0",
         "18.787168499604757",
         null,
         "160M",
         "768",
         "12",
         "12",
         "2.667",
         "64",
         "2048",
         null,
         "64",
         null,
         "2.934087425440974"
        ],
        [
         "649",
         "tokenparam_ctx8192",
         "dclm_llama_400M_ctx8192_lr0.003_steps10000_gbs128",
         "scl_llama_400M",
         "llama",
         "406635520.0",
         "10485760000.0",
         "3.525724694642688e+19",
         "2.9618798267256636",
         "25.78663073998061",
         "42.666666666666664",
         "22",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "10000.0",
         "19.33428271702499",
         null,
         "400M",
         "1024",
         "24",
         "16",
         "2.667",
         "64",
         "2752",
         null,
         "64",
         null,
         "2.9666521931581054"
        ],
        [
         "650",
         "tokenparam_ctx8192",
         "dclm_llama_400M_ctx8192_lr0.003_steps18000_gbs128",
         "scl_llama_400M",
         "llama",
         "406635520.0",
         "18874368000.0",
         "6.346304450356838e+19",
         "2.8525087117293824",
         "46.415935331965095",
         "42.666666666666664",
         "44",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "18000.0",
         "17.331206348657638",
         null,
         "400M",
         "1024",
         "24",
         "16",
         "2.667",
         "64",
         "2752",
         null,
         "64",
         null,
         "2.853139570401379"
        ],
        [
         "651",
         "tokenparam_ctx8192",
         "dclm_llama_400M_ctx8192_lr0.003_steps46000_gbs128",
         "scl_llama_400M",
         "llama",
         "406635520.0",
         "48234496000.0",
         "1.6218333595356365e+20",
         "2.760690410091532",
         "118.6185014039108",
         "42.666666666666664",
         "110",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "46000.0",
         "15.810755085767974",
         null,
         "400M",
         "1024",
         "24",
         "16",
         "2.667",
         "64",
         "2752",
         null,
         "64",
         null,
         "2.7610250746891403"
        ],
        [
         "652",
         "tokenparam_ctx8192",
         "dclm_llama_400M_ctx8192_lr0.003_steps87000_gbs128",
         "scl_llama_400M",
         "llama",
         "406635520.0",
         "91226112000.0",
         "3.0673804843391386e+20",
         "2.7187932375470867",
         "224.3436874378313",
         "42.666666666666664",
         "220",
         "tokenparam",
         "8192",
         "0.003",
         "128.0",
         "87000.0",
         "15.162014250967216",
         null,
         "400M",
         "1024",
         "24",
         "16",
         "2.667",
         "64",
         "2752",
         null,
         "64",
         null,
         "2.7186261164252303"
        ],
        [
         "653",
         "tokenparam_ctx8192",
         "dclm_llama_400M_ctx8192_lr0.001_steps427000_gbs128",
         "scl_llama_400M",
         "llama",
         "406635520.0",
         "447741952000.0",
         "1.5054844446124278e+21",
         "2.648728357390982",
         "1101.089132597172",
         "42.666666666666664",
         "1100",
         "tokenparam",
         "8192",
         "0.001",
         "128.0",
         "427000.0",
         "14.136051205965764",
         null,
         "400M",
         "1024",
         "24",
         "16",
         "2.667",
         "64",
         "2752",
         null,
         "64",
         null,
         "2.6491030677329137"
        ],
        [
         "654",
         "tokenparam_ctx8192",
         "dclm_llama_400M_ctx8192_lr0.001_steps215000_gbs128",
         "scl_llama_400M",
         "llama",
         "406635520.0",
         "225443840000.0",
         "7.580308093481779e+20",
         "2.675237603491056",
         "554.4125609095831",
         "42.666666666666664",
         "550",
         "tokenparam",
         "8192",
         "0.001",
         "128.0",
         "215000.0",
         "14.515798434259068",
         null,
         "400M",
         "1024",
         "24",
         "16",
         "2.667",
         "64",
         "2752",
         null,
         "64",
         null,
         "2.6752704872691875"
        ],
        [
         "655",
         "tokenparam_ctx8192",
         "dclm_llama_830M_ctx8192_lr0.001_steps10000_gbs256",
         "scl_llama_830Mv2",
         "llama",
         "834086400.0",
         "20971520000.0",
         "1.3372672733872128e+20",
         "2.778400269779074",
         "25.14310268096926",
         "64.0",
         "22",
         "tokenparam",
         "8192",
         "0.001",
         "256.0",
         "10000.0",
         "16.093255477480973",
         null,
         "830M",
         "1536",
         "24",
         "16",
         "2.667",
         "64",
         "4096",
         null,
         "96",
         null,
         "2.779872941316842"
        ],
        [
         "656",
         "tokenparam_ctx8192",
         "dclm_llama_830M_ctx8192_lr0.001_steps18000_gbs256",
         "scl_llama_830Mv2",
         "llama",
         "834086400.0",
         "37748736000.0",
         "2.407081092096983e+20",
         "2.6989141345979375",
         "45.25758482574467",
         "64.0",
         "44",
         "tokenparam",
         "8192",
         "0.001",
         "256.0",
         "18000.0",
         "14.863583108199633",
         null,
         "830M",
         "1536",
         "24",
         "16",
         "2.667",
         "64",
         "4096",
         null,
         "96",
         null,
         "2.6981525068268026"
        ],
        [
         "657",
         "tokenparam_ctx8192",
         "dclm_llama_830M_ctx8192_lr0.001_steps46000_gbs256",
         "scl_llama_830Mv2",
         "llama",
         "834086400.0",
         "96468992000.0",
         "6.151429457581179e+20",
         "2.6034393326455376",
         "115.65827233245861",
         "64.0",
         "110",
         "tokenparam",
         "8192",
         "0.001",
         "256.0",
         "46000.0",
         "13.510124031467386",
         null,
         "830M",
         "1536",
         "24",
         "16",
         "2.667",
         "64",
         "4096",
         null,
         "96",
         null,
         "2.603838803555452"
        ],
        [
         "658",
         "tokenparam_ctx8192",
         "dclm_llama_830M_ctx8192_lr0.001_steps90000_gbs256",
         "scl_llama_830Mv2",
         "llama",
         "834086400.0",
         "188743680000.0",
         "1.2035405460484915e+21",
         "2.5519732248941773",
         "226.28792412872335",
         "64.0",
         "220",
         "tokenparam",
         "8192",
         "0.001",
         "256.0",
         "90000.0",
         "12.832400028052039",
         null,
         "830M",
         "1536",
         "24",
         "16",
         "2.667",
         "64",
         "4096",
         null,
         "96",
         null,
         "2.5521807751314975"
        ],
        [
         "659",
         "tokenparam_ctx8192",
         "dclm_llama_830M_ctx8192_lr0.001_steps220000_gbs256",
         "scl_llama_830Mv2",
         "llama",
         "834086400.0",
         "461373440000.0",
         "2.941988001451868e+21",
         "2.5017227559261315",
         "553.1482589813238",
         "64.0",
         "550",
         "tokenparam",
         "8192",
         "0.001",
         "256.0",
         "220000.0",
         "12.203499512893918",
         null,
         "830M",
         "1536",
         "24",
         "16",
         "2.667",
         "64",
         "4096",
         null,
         "96",
         null,
         "2.504537409146656"
        ],
        [
         "660",
         "tokenparam_ctx8192",
         "dclm_llama_1.4B_ctx8192_lr0.0008_steps16000_gbs256",
         "scl_llama_1.4Bv2",
         "llama",
         "1420396544.0",
         "33554432000.0",
         "3.470852407217357e+20",
         "2.634025030963804",
         "23.623284738152673",
         "85.33333333333333",
         "22",
         "tokenparam",
         "8192",
         "0.0008",
         "256.0",
         "16000.0",
         "13.929724790105851",
         null,
         "1.4B",
         "2048",
         "24",
         "16",
         "2.667",
         "64",
         "5504",
         null,
         "128",
         null,
         "2.6331738346192783"
        ],
        [
         "661",
         "tokenparam_ctx8192",
         "dclm_llama_1.4B_ctx8192_lr0.0008_steps31000_gbs256",
         "scl_llama_1.4Bv2",
         "llama",
         "1420396544.0",
         "65011712000.0",
         "6.724776538983629e+20",
         "2.5629621115844152",
         "45.7701141801708",
         "85.33333333333333",
         "44",
         "tokenparam",
         "8192",
         "0.0008",
         "256.0",
         "31000.0",
         "12.974191456052173",
         null,
         "1.4B",
         "2048",
         "24",
         "16",
         "2.667",
         "64",
         "5504",
         null,
         "128",
         null,
         "2.5627564006301324"
        ],
        [
         "662",
         "tokenparam_ctx8192",
         "dclm_llama_1.4B_ctx8192_lr0.0008_steps76000_gbs256",
         "scl_llama_1.4Bv2",
         "llama",
         "1420396544.0",
         "159383552000.0",
         "1.6486548934282445e+21",
         "2.480129212399403",
         "112.21060250622519",
         "85.33333333333333",
         "110",
         "tokenparam",
         "8192",
         "0.0008",
         "256.0",
         "76000.0",
         "11.94280747696548",
         null,
         "1.4B",
         "2048",
         "24",
         "16",
         "2.667",
         "64",
         "5504",
         null,
         "128",
         null,
         "2.4774110424170694"
        ],
        [
         "663",
         "tokenparam_ctx8192",
         "dclm_llama_1.4B_ctx8192_lr0.0008_steps150000_gbs256",
         "scl_llama_1.4Bv2",
         "llama",
         "1420396544.0",
         "314572800000.0",
         "3.253924131766272e+21",
         "2.436433086134944",
         "221.4682944201813",
         "85.33333333333333",
         "220",
         "tokenparam",
         "8192",
         "0.0008",
         "256.0",
         "150000.0",
         "11.432190293024101",
         null,
         "1.4B",
         "2048",
         "24",
         "16",
         "2.667",
         "64",
         "5504",
         null,
         "128",
         null,
         "2.4349899866365776"
        ],
        [
         "664",
         "tokenparam_ctx8192",
         "dclm_llama_1.4B_ctx8192_lr0.0008_steps375000_gbs256",
         "scl_llama_1.4Bv2",
         "llama",
         "1420396544.0",
         "786432000000.0",
         "8.13481032941568e+21",
         "2.39036253263936",
         "553.6707360504532",
         "85.33333333333333",
         "550",
         "tokenparam",
         "8192",
         "0.0008",
         "256.0",
         "375000.0",
         "10.917451158072245",
         null,
         "1.4B",
         "2048",
         "24",
         "16",
         "2.667",
         "64",
         "5504",
         null,
         "128",
         null,
         "2.3906979181469326"
        ],
        [
         "665",
         "tokenparam_ctx8192",
         "dclm_llama_2.7B_ctx8192_lr0.0007_steps16000_gbs512",
         "scl_llama_2.7B",
         "llama",
         "2779548160.0",
         "67108864000.0",
         "1.3418002849506263e+21",
         "2.485857815586063",
         "24.1438032863586",
         "80.0",
         "22",
         "tokenparam",
         "8192",
         "0.0007",
         "512.0",
         "16000.0",
         "12.011419419596484",
         null,
         "2.7B",
         "2560",
         "32",
         "32",
         "2.667",
         "64",
         "6848",
         null,
         "80",
         null,
         "2.488384306647576"
        ],
        [
         "666",
         "tokenparam_ctx8192",
         "dclm_llama_2.7B_ctx8192_lr0.0007_steps31000_gbs512",
         "scl_llama_2.7B",
         "llama",
         "2779548160.0",
         "130023424000.0",
         "2.5997380520918385e+21",
         "2.4072120541077937",
         "46.778618867319786",
         "80.0",
         "44",
         "tokenparam",
         "8192",
         "0.0007",
         "512.0",
         "31000.0",
         "11.102963493792005",
         null,
         "2.7B",
         "2560",
         "32",
         "32",
         "2.667",
         "64",
         "6848",
         null,
         "80",
         null,
         "2.4080170311996487"
        ],
        [
         "667",
         "tokenparam_ctx8192",
         "dclm_llama_2.7B_ctx8192_lr0.0007_steps76000_gbs512",
         "scl_llama_2.7B",
         "llama",
         "2779548160.0",
         "318767104000.0",
         "6.373551353515475e+21",
         "2.3363157959069967",
         "114.68306561020336",
         "80.0",
         "110",
         "tokenparam",
         "8192",
         "0.0007",
         "512.0",
         "76000.0",
         "10.343060336183672",
         null,
         "2.7B",
         "2560",
         "32",
         "32",
         "2.667",
         "64",
         "6848",
         null,
         "80",
         null,
         "2.33742677597639"
        ],
        [
         "668",
         "tokenparam_ctx8192",
         "dclm_llama_2.7B_ctx8192_lr0.0007_steps146000_gbs512",
         "scl_llama_2.7B",
         "llama",
         "2779548160.0",
         "612368384000.0",
         "1.2243927600174465e+22",
         "2.294342841200091",
         "220.31220498802222",
         "80.0",
         "220",
         "tokenparam",
         "8192",
         "0.0007",
         "512.0",
         "146000.0",
         "9.91791622432839",
         null,
         "2.7B",
         "2560",
         "32",
         "32",
         "2.667",
         "64",
         "6848",
         null,
         "80",
         null,
         "2.293954715254618"
        ],
        [
         "669",
         "tokenparam_ctx8192",
         "dclm_llama_7B_ctx8192_lr0.0005_steps76000_gbs256",
         "scl_llama_7B",
         "llama",
         "6863196160.0",
         "159383552000.0",
         "7.403170772325237e+21",
         "2.2784035293820932",
         "23.222934079739314",
         "128.0",
         "22",
         "tokenparam",
         "8192",
         "0.0005",
         "256.0",
         "76000.0",
         "9.76108467888972",
         null,
         "7B",
         "4096",
         "32",
         "32",
         "2.667",
         "64",
         "10944",
         null,
         "128",
         null,
         "2.2768505612715955"
        ],
        [
         "670",
         "tokenparam_ctx8192",
         "dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs256",
         "scl_llama_7B",
         "llama",
         "6863196160.0",
         "304087040000.0",
         "1.4124470552462623e+22",
         "2.22428963359096",
         "44.3069137047658",
         "128.0",
         "44",
         "tokenparam",
         "8192",
         "0.0005",
         "256.0",
         "145000.0",
         "9.246911774751881",
         null,
         "7B",
         "4096",
         "32",
         "32",
         "2.667",
         "64",
         "10944",
         null,
         "128",
         null,
         "2.227857393654961"
        ],
        [
         "671",
         "tokenparam_ctx8192",
         "dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs512",
         "scl_llama_7B",
         "llama",
         "6863196160.0",
         "608174080000.0",
         "2.8248941104925245e+22",
         "2.181153654704367",
         "88.6138274095316",
         "128.0",
         "extra",
         "tokenparam",
         "8192",
         "0.0005",
         "512.0",
         "145000.0",
         "8.856517730676302",
         null,
         "7B",
         "4096",
         "32",
         "32",
         "2.667",
         "64",
         "10944",
         null,
         "128",
         null,
         "2.181395989524274"
        ]
       ],
       "shape": {
        "columns": 29,
        "rows": 31
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_set_ctx_length</th>\n",
       "      <th>name</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>model_type</th>\n",
       "      <th>num_params</th>\n",
       "      <th>num_tokens_training</th>\n",
       "      <th>num_flops_training</th>\n",
       "      <th>val/.dclm_loss</th>\n",
       "      <th>token_param_ratio</th>\n",
       "      <th>width_depth_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>proj_factor_ffn</th>\n",
       "      <th>ffn_multiple_of</th>\n",
       "      <th>ffn_dim</th>\n",
       "      <th>head_dim_qk</th>\n",
       "      <th>head_dim_v</th>\n",
       "      <th>IsoFLOP</th>\n",
       "      <th>train/.loss_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps3500_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>3.670016e+09</td>\n",
       "      <td>4.416455e+18</td>\n",
       "      <td>3.298485</td>\n",
       "      <td>22.623585</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.300660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps5000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>5.242880e+09</td>\n",
       "      <td>6.309221e+18</td>\n",
       "      <td>3.220001</td>\n",
       "      <td>32.319407</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.217189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps7000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>7.340032e+09</td>\n",
       "      <td>8.832909e+18</td>\n",
       "      <td>3.162786</td>\n",
       "      <td>45.247169</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.161473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps8000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>8.388608e+09</td>\n",
       "      <td>1.009475e+19</td>\n",
       "      <td>3.143749</td>\n",
       "      <td>51.711051</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.142752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps18000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>1.887437e+10</td>\n",
       "      <td>2.271320e+19</td>\n",
       "      <td>3.050733</td>\n",
       "      <td>116.349864</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.051504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps36000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>3.774874e+10</td>\n",
       "      <td>4.542639e+19</td>\n",
       "      <td>2.995063</td>\n",
       "      <td>232.699728</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.995211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.003_steps87000_gbs128</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>9.122611e+10</td>\n",
       "      <td>1.097804e+20</td>\n",
       "      <td>2.946427</td>\n",
       "      <td>562.357675</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.946148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_160M_ctx8192_lr0.001_steps173000_gb...</td>\n",
       "      <td>scl_llama_160M</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.622208e+08</td>\n",
       "      <td>1.814036e+11</td>\n",
       "      <td>2.182990e+20</td>\n",
       "      <td>2.933174</td>\n",
       "      <td>1118.251470</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>768</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.934087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps10000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>1.048576e+10</td>\n",
       "      <td>3.525725e+19</td>\n",
       "      <td>2.961880</td>\n",
       "      <td>25.786631</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.966652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps18000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>1.887437e+10</td>\n",
       "      <td>6.346304e+19</td>\n",
       "      <td>2.852509</td>\n",
       "      <td>46.415935</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.853140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps46000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>4.823450e+10</td>\n",
       "      <td>1.621833e+20</td>\n",
       "      <td>2.760690</td>\n",
       "      <td>118.618501</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.761025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.003_steps87000_gbs128</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>9.122611e+10</td>\n",
       "      <td>3.067380e+20</td>\n",
       "      <td>2.718793</td>\n",
       "      <td>224.343687</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.718626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.001_steps427000_gb...</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>4.477420e+11</td>\n",
       "      <td>1.505484e+21</td>\n",
       "      <td>2.648728</td>\n",
       "      <td>1101.089133</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.649103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_400M_ctx8192_lr0.001_steps215000_gb...</td>\n",
       "      <td>scl_llama_400M</td>\n",
       "      <td>llama</td>\n",
       "      <td>4.066355e+08</td>\n",
       "      <td>2.254438e+11</td>\n",
       "      <td>7.580308e+20</td>\n",
       "      <td>2.675238</td>\n",
       "      <td>554.412561</td>\n",
       "      <td>42.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.675270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps10000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>2.097152e+10</td>\n",
       "      <td>1.337267e+20</td>\n",
       "      <td>2.778400</td>\n",
       "      <td>25.143103</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1536</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.779873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps18000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>3.774874e+10</td>\n",
       "      <td>2.407081e+20</td>\n",
       "      <td>2.698914</td>\n",
       "      <td>45.257585</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1536</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.698153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps46000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>9.646899e+10</td>\n",
       "      <td>6.151429e+20</td>\n",
       "      <td>2.603439</td>\n",
       "      <td>115.658272</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1536</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.603839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps90000_gbs256</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>1.887437e+11</td>\n",
       "      <td>1.203541e+21</td>\n",
       "      <td>2.551973</td>\n",
       "      <td>226.287924</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1536</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.552181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_830M_ctx8192_lr0.001_steps220000_gb...</td>\n",
       "      <td>scl_llama_830Mv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>8.340864e+08</td>\n",
       "      <td>4.613734e+11</td>\n",
       "      <td>2.941988e+21</td>\n",
       "      <td>2.501723</td>\n",
       "      <td>553.148259</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1536</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>4096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.504537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps16000_gb...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>3.355443e+10</td>\n",
       "      <td>3.470852e+20</td>\n",
       "      <td>2.634025</td>\n",
       "      <td>23.623285</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.633174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps31000_gb...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>6.501171e+10</td>\n",
       "      <td>6.724777e+20</td>\n",
       "      <td>2.562962</td>\n",
       "      <td>45.770114</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.562756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps76000_gb...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>1.593836e+11</td>\n",
       "      <td>1.648655e+21</td>\n",
       "      <td>2.480129</td>\n",
       "      <td>112.210603</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.477411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps150000_g...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>3.145728e+11</td>\n",
       "      <td>3.253924e+21</td>\n",
       "      <td>2.436433</td>\n",
       "      <td>221.468294</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.434990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_1.4B_ctx8192_lr0.0008_steps375000_g...</td>\n",
       "      <td>scl_llama_1.4Bv2</td>\n",
       "      <td>llama</td>\n",
       "      <td>1.420397e+09</td>\n",
       "      <td>7.864320e+11</td>\n",
       "      <td>8.134810e+21</td>\n",
       "      <td>2.390363</td>\n",
       "      <td>553.670736</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2048</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>5504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.390698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps16000_gb...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>6.710886e+10</td>\n",
       "      <td>1.341800e+21</td>\n",
       "      <td>2.485858</td>\n",
       "      <td>24.143803</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2560</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.488384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps31000_gb...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>1.300234e+11</td>\n",
       "      <td>2.599738e+21</td>\n",
       "      <td>2.407212</td>\n",
       "      <td>46.778619</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2560</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.408017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps76000_gb...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>3.187671e+11</td>\n",
       "      <td>6.373551e+21</td>\n",
       "      <td>2.336316</td>\n",
       "      <td>114.683066</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2560</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.337427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_2.7B_ctx8192_lr0.0007_steps146000_g...</td>\n",
       "      <td>scl_llama_2.7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>2.779548e+09</td>\n",
       "      <td>6.123684e+11</td>\n",
       "      <td>1.224393e+22</td>\n",
       "      <td>2.294343</td>\n",
       "      <td>220.312205</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2560</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>6848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.293955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_7B_ctx8192_lr0.0005_steps76000_gbs256</td>\n",
       "      <td>scl_llama_7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>6.863196e+09</td>\n",
       "      <td>1.593836e+11</td>\n",
       "      <td>7.403171e+21</td>\n",
       "      <td>2.278404</td>\n",
       "      <td>23.222934</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>10944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.276851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs256</td>\n",
       "      <td>scl_llama_7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>6.863196e+09</td>\n",
       "      <td>3.040870e+11</td>\n",
       "      <td>1.412447e+22</td>\n",
       "      <td>2.224290</td>\n",
       "      <td>44.306914</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>10944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.227857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs512</td>\n",
       "      <td>scl_llama_7B</td>\n",
       "      <td>llama</td>\n",
       "      <td>6.863196e+09</td>\n",
       "      <td>6.081741e+11</td>\n",
       "      <td>2.824894e+22</td>\n",
       "      <td>2.181154</td>\n",
       "      <td>88.613827</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>2.667</td>\n",
       "      <td>64</td>\n",
       "      <td>10944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.181396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_set_ctx_length  \\\n",
       "641        tokenparam_ctx8192   \n",
       "642        tokenparam_ctx8192   \n",
       "643        tokenparam_ctx8192   \n",
       "644        tokenparam_ctx8192   \n",
       "645        tokenparam_ctx8192   \n",
       "646        tokenparam_ctx8192   \n",
       "647        tokenparam_ctx8192   \n",
       "648        tokenparam_ctx8192   \n",
       "649        tokenparam_ctx8192   \n",
       "650        tokenparam_ctx8192   \n",
       "651        tokenparam_ctx8192   \n",
       "652        tokenparam_ctx8192   \n",
       "653        tokenparam_ctx8192   \n",
       "654        tokenparam_ctx8192   \n",
       "655        tokenparam_ctx8192   \n",
       "656        tokenparam_ctx8192   \n",
       "657        tokenparam_ctx8192   \n",
       "658        tokenparam_ctx8192   \n",
       "659        tokenparam_ctx8192   \n",
       "660        tokenparam_ctx8192   \n",
       "661        tokenparam_ctx8192   \n",
       "662        tokenparam_ctx8192   \n",
       "663        tokenparam_ctx8192   \n",
       "664        tokenparam_ctx8192   \n",
       "665        tokenparam_ctx8192   \n",
       "666        tokenparam_ctx8192   \n",
       "667        tokenparam_ctx8192   \n",
       "668        tokenparam_ctx8192   \n",
       "669        tokenparam_ctx8192   \n",
       "670        tokenparam_ctx8192   \n",
       "671        tokenparam_ctx8192   \n",
       "\n",
       "                                                  name           run_tag  \\\n",
       "641   dclm_llama_160M_ctx8192_lr0.003_steps3500_gbs128    scl_llama_160M   \n",
       "642   dclm_llama_160M_ctx8192_lr0.003_steps5000_gbs128    scl_llama_160M   \n",
       "643   dclm_llama_160M_ctx8192_lr0.003_steps7000_gbs128    scl_llama_160M   \n",
       "644   dclm_llama_160M_ctx8192_lr0.003_steps8000_gbs128    scl_llama_160M   \n",
       "645  dclm_llama_160M_ctx8192_lr0.003_steps18000_gbs128    scl_llama_160M   \n",
       "646  dclm_llama_160M_ctx8192_lr0.003_steps36000_gbs128    scl_llama_160M   \n",
       "647  dclm_llama_160M_ctx8192_lr0.003_steps87000_gbs128    scl_llama_160M   \n",
       "648  dclm_llama_160M_ctx8192_lr0.001_steps173000_gb...    scl_llama_160M   \n",
       "649  dclm_llama_400M_ctx8192_lr0.003_steps10000_gbs128    scl_llama_400M   \n",
       "650  dclm_llama_400M_ctx8192_lr0.003_steps18000_gbs128    scl_llama_400M   \n",
       "651  dclm_llama_400M_ctx8192_lr0.003_steps46000_gbs128    scl_llama_400M   \n",
       "652  dclm_llama_400M_ctx8192_lr0.003_steps87000_gbs128    scl_llama_400M   \n",
       "653  dclm_llama_400M_ctx8192_lr0.001_steps427000_gb...    scl_llama_400M   \n",
       "654  dclm_llama_400M_ctx8192_lr0.001_steps215000_gb...    scl_llama_400M   \n",
       "655  dclm_llama_830M_ctx8192_lr0.001_steps10000_gbs256  scl_llama_830Mv2   \n",
       "656  dclm_llama_830M_ctx8192_lr0.001_steps18000_gbs256  scl_llama_830Mv2   \n",
       "657  dclm_llama_830M_ctx8192_lr0.001_steps46000_gbs256  scl_llama_830Mv2   \n",
       "658  dclm_llama_830M_ctx8192_lr0.001_steps90000_gbs256  scl_llama_830Mv2   \n",
       "659  dclm_llama_830M_ctx8192_lr0.001_steps220000_gb...  scl_llama_830Mv2   \n",
       "660  dclm_llama_1.4B_ctx8192_lr0.0008_steps16000_gb...  scl_llama_1.4Bv2   \n",
       "661  dclm_llama_1.4B_ctx8192_lr0.0008_steps31000_gb...  scl_llama_1.4Bv2   \n",
       "662  dclm_llama_1.4B_ctx8192_lr0.0008_steps76000_gb...  scl_llama_1.4Bv2   \n",
       "663  dclm_llama_1.4B_ctx8192_lr0.0008_steps150000_g...  scl_llama_1.4Bv2   \n",
       "664  dclm_llama_1.4B_ctx8192_lr0.0008_steps375000_g...  scl_llama_1.4Bv2   \n",
       "665  dclm_llama_2.7B_ctx8192_lr0.0007_steps16000_gb...    scl_llama_2.7B   \n",
       "666  dclm_llama_2.7B_ctx8192_lr0.0007_steps31000_gb...    scl_llama_2.7B   \n",
       "667  dclm_llama_2.7B_ctx8192_lr0.0007_steps76000_gb...    scl_llama_2.7B   \n",
       "668  dclm_llama_2.7B_ctx8192_lr0.0007_steps146000_g...    scl_llama_2.7B   \n",
       "669   dclm_llama_7B_ctx8192_lr0.0005_steps76000_gbs256      scl_llama_7B   \n",
       "670  dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs256      scl_llama_7B   \n",
       "671  dclm_llama_7B_ctx8192_lr0.0005_steps145000_gbs512      scl_llama_7B   \n",
       "\n",
       "    model_type    num_params  num_tokens_training  num_flops_training  \\\n",
       "641      llama  1.622208e+08         3.670016e+09        4.416455e+18   \n",
       "642      llama  1.622208e+08         5.242880e+09        6.309221e+18   \n",
       "643      llama  1.622208e+08         7.340032e+09        8.832909e+18   \n",
       "644      llama  1.622208e+08         8.388608e+09        1.009475e+19   \n",
       "645      llama  1.622208e+08         1.887437e+10        2.271320e+19   \n",
       "646      llama  1.622208e+08         3.774874e+10        4.542639e+19   \n",
       "647      llama  1.622208e+08         9.122611e+10        1.097804e+20   \n",
       "648      llama  1.622208e+08         1.814036e+11        2.182990e+20   \n",
       "649      llama  4.066355e+08         1.048576e+10        3.525725e+19   \n",
       "650      llama  4.066355e+08         1.887437e+10        6.346304e+19   \n",
       "651      llama  4.066355e+08         4.823450e+10        1.621833e+20   \n",
       "652      llama  4.066355e+08         9.122611e+10        3.067380e+20   \n",
       "653      llama  4.066355e+08         4.477420e+11        1.505484e+21   \n",
       "654      llama  4.066355e+08         2.254438e+11        7.580308e+20   \n",
       "655      llama  8.340864e+08         2.097152e+10        1.337267e+20   \n",
       "656      llama  8.340864e+08         3.774874e+10        2.407081e+20   \n",
       "657      llama  8.340864e+08         9.646899e+10        6.151429e+20   \n",
       "658      llama  8.340864e+08         1.887437e+11        1.203541e+21   \n",
       "659      llama  8.340864e+08         4.613734e+11        2.941988e+21   \n",
       "660      llama  1.420397e+09         3.355443e+10        3.470852e+20   \n",
       "661      llama  1.420397e+09         6.501171e+10        6.724777e+20   \n",
       "662      llama  1.420397e+09         1.593836e+11        1.648655e+21   \n",
       "663      llama  1.420397e+09         3.145728e+11        3.253924e+21   \n",
       "664      llama  1.420397e+09         7.864320e+11        8.134810e+21   \n",
       "665      llama  2.779548e+09         6.710886e+10        1.341800e+21   \n",
       "666      llama  2.779548e+09         1.300234e+11        2.599738e+21   \n",
       "667      llama  2.779548e+09         3.187671e+11        6.373551e+21   \n",
       "668      llama  2.779548e+09         6.123684e+11        1.224393e+22   \n",
       "669      llama  6.863196e+09         1.593836e+11        7.403171e+21   \n",
       "670      llama  6.863196e+09         3.040870e+11        1.412447e+22   \n",
       "671      llama  6.863196e+09         6.081741e+11        2.824894e+22   \n",
       "\n",
       "     val/.dclm_loss  token_param_ratio  width_depth_ratio  ... embedding_dim  \\\n",
       "641        3.298485          22.623585          64.000000  ...           768   \n",
       "642        3.220001          32.319407          64.000000  ...           768   \n",
       "643        3.162786          45.247169          64.000000  ...           768   \n",
       "644        3.143749          51.711051          64.000000  ...           768   \n",
       "645        3.050733         116.349864          64.000000  ...           768   \n",
       "646        2.995063         232.699728          64.000000  ...           768   \n",
       "647        2.946427         562.357675          64.000000  ...           768   \n",
       "648        2.933174        1118.251470          64.000000  ...           768   \n",
       "649        2.961880          25.786631          42.666667  ...          1024   \n",
       "650        2.852509          46.415935          42.666667  ...          1024   \n",
       "651        2.760690         118.618501          42.666667  ...          1024   \n",
       "652        2.718793         224.343687          42.666667  ...          1024   \n",
       "653        2.648728        1101.089133          42.666667  ...          1024   \n",
       "654        2.675238         554.412561          42.666667  ...          1024   \n",
       "655        2.778400          25.143103          64.000000  ...          1536   \n",
       "656        2.698914          45.257585          64.000000  ...          1536   \n",
       "657        2.603439         115.658272          64.000000  ...          1536   \n",
       "658        2.551973         226.287924          64.000000  ...          1536   \n",
       "659        2.501723         553.148259          64.000000  ...          1536   \n",
       "660        2.634025          23.623285          85.333333  ...          2048   \n",
       "661        2.562962          45.770114          85.333333  ...          2048   \n",
       "662        2.480129         112.210603          85.333333  ...          2048   \n",
       "663        2.436433         221.468294          85.333333  ...          2048   \n",
       "664        2.390363         553.670736          85.333333  ...          2048   \n",
       "665        2.485858          24.143803          80.000000  ...          2560   \n",
       "666        2.407212          46.778619          80.000000  ...          2560   \n",
       "667        2.336316         114.683066          80.000000  ...          2560   \n",
       "668        2.294343         220.312205          80.000000  ...          2560   \n",
       "669        2.278404          23.222934         128.000000  ...          4096   \n",
       "670        2.224290          44.306914         128.000000  ...          4096   \n",
       "671        2.181154          88.613827         128.000000  ...          4096   \n",
       "\n",
       "    num_blocks  num_heads  proj_factor_ffn  ffn_multiple_of  ffn_dim  \\\n",
       "641         12         12            2.667               64     2048   \n",
       "642         12         12            2.667               64     2048   \n",
       "643         12         12            2.667               64     2048   \n",
       "644         12         12            2.667               64     2048   \n",
       "645         12         12            2.667               64     2048   \n",
       "646         12         12            2.667               64     2048   \n",
       "647         12         12            2.667               64     2048   \n",
       "648         12         12            2.667               64     2048   \n",
       "649         24         16            2.667               64     2752   \n",
       "650         24         16            2.667               64     2752   \n",
       "651         24         16            2.667               64     2752   \n",
       "652         24         16            2.667               64     2752   \n",
       "653         24         16            2.667               64     2752   \n",
       "654         24         16            2.667               64     2752   \n",
       "655         24         16            2.667               64     4096   \n",
       "656         24         16            2.667               64     4096   \n",
       "657         24         16            2.667               64     4096   \n",
       "658         24         16            2.667               64     4096   \n",
       "659         24         16            2.667               64     4096   \n",
       "660         24         16            2.667               64     5504   \n",
       "661         24         16            2.667               64     5504   \n",
       "662         24         16            2.667               64     5504   \n",
       "663         24         16            2.667               64     5504   \n",
       "664         24         16            2.667               64     5504   \n",
       "665         32         32            2.667               64     6848   \n",
       "666         32         32            2.667               64     6848   \n",
       "667         32         32            2.667               64     6848   \n",
       "668         32         32            2.667               64     6848   \n",
       "669         32         32            2.667               64    10944   \n",
       "670         32         32            2.667               64    10944   \n",
       "671         32         32            2.667               64    10944   \n",
       "\n",
       "     head_dim_qk head_dim_v IsoFLOP  train/.loss_mean  \n",
       "641          NaN         64     NaN          3.300660  \n",
       "642          NaN         64     NaN          3.217189  \n",
       "643          NaN         64     NaN          3.161473  \n",
       "644          NaN         64     NaN          3.142752  \n",
       "645          NaN         64     NaN          3.051504  \n",
       "646          NaN         64     NaN          2.995211  \n",
       "647          NaN         64     NaN          2.946148  \n",
       "648          NaN         64     NaN          2.934087  \n",
       "649          NaN         64     NaN          2.966652  \n",
       "650          NaN         64     NaN          2.853140  \n",
       "651          NaN         64     NaN          2.761025  \n",
       "652          NaN         64     NaN          2.718626  \n",
       "653          NaN         64     NaN          2.649103  \n",
       "654          NaN         64     NaN          2.675270  \n",
       "655          NaN         96     NaN          2.779873  \n",
       "656          NaN         96     NaN          2.698153  \n",
       "657          NaN         96     NaN          2.603839  \n",
       "658          NaN         96     NaN          2.552181  \n",
       "659          NaN         96     NaN          2.504537  \n",
       "660          NaN        128     NaN          2.633174  \n",
       "661          NaN        128     NaN          2.562756  \n",
       "662          NaN        128     NaN          2.477411  \n",
       "663          NaN        128     NaN          2.434990  \n",
       "664          NaN        128     NaN          2.390698  \n",
       "665          NaN         80     NaN          2.488384  \n",
       "666          NaN         80     NaN          2.408017  \n",
       "667          NaN         80     NaN          2.337427  \n",
       "668          NaN         80     NaN          2.293955  \n",
       "669          NaN        128     NaN          2.276851  \n",
       "670          NaN        128     NaN          2.227857  \n",
       "671          NaN        128     NaN          2.181396  \n",
       "\n",
       "[31 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "df[(df[\"experiment_set\"] == \"tokenparam\") & (df[\"model_type\"] == \"llama\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcc26bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.184992444075694e+23\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_flops_training",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "64f5e61e-4a33-4b4b-aa22-9dbfab8f1f17",
       "rows": [
        [
         "count",
         "672.0"
        ],
        [
         "mean",
         "4.7395720893983536e+20"
        ],
        [
         "std",
         "3.841667329804221e+21"
        ],
        [
         "min",
         "2.809832926150656e+18"
        ],
        [
         "25%",
         "1.0003848697479168e+19"
        ],
        [
         "50%",
         "2.9975036722937856e+19"
        ],
        [
         "75%",
         "9.859802558072095e+19"
        ],
        [
         "max",
         "8.4809675798741e+22"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    6.720000e+02\n",
       "mean     4.739572e+20\n",
       "std      3.841667e+21\n",
       "min      2.809833e+18\n",
       "25%      1.000385e+19\n",
       "50%      2.997504e+19\n",
       "75%      9.859803e+19\n",
       "max      8.480968e+22\n",
       "Name: num_flops_training, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get min FLOPs, max FLOPs, min train tokens, max train tokens, min params, max params\n",
    "df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "print(df[\"num_flops_training\"].sum())\n",
    "df[\"num_flops_training\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f48ae94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_tokens_training",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1a584576-b26e-4ee1-988f-fb57415caf7c",
       "rows": [
        [
         "count",
         "672.0"
        ],
        [
         "mean",
         "37369641447.61905"
        ],
        [
         "std",
         "134236129060.1476"
        ],
        [
         "min",
         "1887436800.0"
        ],
        [
         "25%",
         "4404019200.0"
        ],
        [
         "50%",
         "8808038400.0"
        ],
        [
         "75%",
         "19922944000.0"
        ],
        [
         "max",
         "2097152000000.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    6.720000e+02\n",
       "mean     3.736964e+10\n",
       "std      1.342361e+11\n",
       "min      1.887437e+09\n",
       "25%      4.404019e+09\n",
       "50%      8.808038e+09\n",
       "75%      1.992294e+10\n",
       "max      2.097152e+12\n",
       "Name: num_tokens_training, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"num_tokens_training\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e7da50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "num_params",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e3bfb5bb-34c5-417f-8d28-1b1f50c9e63b",
       "rows": [
        [
         "count",
         "672.0"
        ],
        [
         "mean",
         "785821625.1666666"
        ],
        [
         "std",
         "1058848479.4175397"
        ],
        [
         "min",
         "83634688.0"
        ],
        [
         "25%",
         "207148928.0"
        ],
        [
         "50%",
         "406856896.0"
        ],
        [
         "75%",
         "858132352.0"
        ],
        [
         "max",
         "6867522560.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    6.720000e+02\n",
       "mean     7.858216e+08\n",
       "std      1.058848e+09\n",
       "min      8.363469e+07\n",
       "25%      2.071489e+08\n",
       "50%      4.068569e+08\n",
       "75%      8.581324e+08\n",
       "max      6.867523e+09\n",
       "Name: num_params, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"num_params\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf4132",
   "metadata": {},
   "source": [
    "## Model Configuration Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1404e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_set_df(exp_set: str | list[str], model_type: str) -> pd.DataFrame:\n",
    "    mlstm_df = get_all_parametric_sclaw_fit_data_dataframe(model_type=model_type)\n",
    "    if model_type == \"mlstm\":\n",
    "        sel_cols = [\n",
    "            \"num_params\",\n",
    "            \"embedding_dim\",\n",
    "            \"ffn_dim\",\n",
    "            \"head_dim_qk\",\n",
    "            \"head_dim_v\",\n",
    "            \"num_heads\",\n",
    "            \"num_blocks\",\n",
    "            # \"context_length\",\n",
    "            # \"global_batch_size\",\n",
    "            # \"learning_rate\",\n",
    "        ]\n",
    "    elif model_type == \"llama\":\n",
    "        sel_cols = [\n",
    "            \"num_params\",\n",
    "            \"embedding_dim\",\n",
    "            \"ffn_dim\",\n",
    "            \"head_dim_v\",\n",
    "            \"num_heads\",\n",
    "            \"num_blocks\",\n",
    "            # \"context_length\",\n",
    "            # \"global_batch_size\",\n",
    "            # \"learning_rate\",\n",
    "        ]\n",
    "    if \"tokenparam\" in exp_set:\n",
    "        sel_cols += [\"global_batch_size\", \"learning_rate\"]\n",
    "    if isinstance(exp_set, str):\n",
    "        exp_set = [exp_set]\n",
    "    exp_set_df = (\n",
    "        mlstm_df[mlstm_df[\"experiment_set_ctx_length\"].isin(exp_set)][sel_cols]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(by=[\"num_params\"])\n",
    "    )\n",
    "    if \"head_dim_qk\" in sel_cols:\n",
    "        exp_set_df[\"head_dim_qk\"] = exp_set_df[\"head_dim_qk\"].astype(int)\n",
    "\n",
    "    if \"global_batch_size\" in sel_cols:\n",
    "        exp_set_df[\"global_batch_size\"] = exp_set_df[\"global_batch_size\"].astype(int)\n",
    "\n",
    "    # convert num_params in millions\n",
    "    exp_set_df[\"num_params\"] = (exp_set_df[\"num_params\"] / 1e6).astype(int)\n",
    "    exp_set_df = exp_set_df.rename(columns={\"num_params\": \"num_params (M)\"})\n",
    "    exp_set_df = exp_set_df.reset_index(drop=True)\n",
    "\n",
    "    # add a \\ before each _ in column names for latex\n",
    "    # exp_set_df.columns = [col.replace(\"_\", \"\\\\_\") for col in exp_set_df.columns]\n",
    "\n",
    "    # prettify column names\n",
    "    if model_type == \"mlstm\":\n",
    "        col_name_map = {\n",
    "            \"num_params (M)\": \"\\#Params (M)\",\n",
    "            \"embedding_dim\": r\"$d_{\\text{model}}$\",\n",
    "            \"ffn_dim\": r\"$d_{\\text{ff}}$\",\n",
    "            \"head_dim_qk\": r\"$d_{\\text{qk}}$\",\n",
    "            \"head_dim_v\": r\"$d_{\\text{hv}}$\",\n",
    "            \"num_heads\": r\"$n_{\\text{heads}}$\",\n",
    "            \"num_blocks\": r\"$n_{\\text{layer}}$\",\n",
    "            \"context_length\": r\"$T$ (ctx)\",\n",
    "            \"global_batch_size\": r\"$B$ (batch)\",\n",
    "            \"learning_rate\": \"LR\",\n",
    "        }\n",
    "    elif model_type == \"llama\":\n",
    "        col_name_map = {\n",
    "            \"num_params (M)\": \"\\#Params (M)\",\n",
    "            \"embedding_dim\": r\"$d_{\\text{model}}$\",\n",
    "            \"ffn_dim\": r\"$d_{\\text{ff}}$\",\n",
    "            \"head_dim_v\": r\"$d_{\\text{v}}$\",\n",
    "            \"num_heads\": r\"$n_{\\text{heads}}$\",\n",
    "            \"num_blocks\": r\"$n_{\\text{layer}}$\",\n",
    "            \"context_length\": r\"$T$ (ctx)\",\n",
    "            \"global_batch_size\": r\"$B$ (batch)\",\n",
    "            \"learning_rate\": \"LR\",\n",
    "        }\n",
    "    exp_set_df = exp_set_df.rename(columns=col_name_map)\n",
    "\n",
    "    return exp_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaee9b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not extract chunk size from backend kwargs '{'backend_name': 'chunkwise--triton_xl_chunk'}' from run WandBRunData(name=dclm_mLSTMv1_160M_ctx8192_lr0.003_steps18000_gbs128, id=djkqz1dc, path=['xlstm', 'xlstm_jax', 'djkqz1dc'], created_at=2024-12-19T15:53:36Z).Using default chunk size of 64.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not extract chunk size from backend kwargs '{'backend_name': 'max_triton_noslice'}' from run WandBRunData(name=dclm_mLSTMv1_7B_ctx8192_gbs512, id=hphy3hsq, path=['xlstm', 'xlstm_jax', 'hphy3hsq'], created_at=2024-11-06T22:21:03Z).Using default chunk size of 64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for xLSTM models trained with the Token/Param configuration.}\n",
      "\\label{tab:tokenparam_hyperparams}\n",
      "\\begin{tabular}{r|rrrrrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{qk}}$ & $d_{\\text{hv}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ & $B$ (batch) & LR \\\\\n",
      "\\midrule\n",
      "164 & 768 & 2112 & 64 & 128 & 6 & 12 & 128 & 3e-3 \\\\\n",
      "\\rowcolor{gray!10}406 & 1024 & 2752 & 128 & 256 & 4 & 24 & 128 & 3e-3 \\\\\n",
      "406 & 1024 & 2752 & 128 & 256 & 4 & 24 & 128 & 1e-3 \\\\\n",
      "\\rowcolor{gray!10}841 & 1536 & 4160 & 192 & 384 & 4 & 24 & 256 & 1e-3 \\\\\n",
      "841 & 1536 & 4160 & 192 & 384 & 4 & 24 & 256 & 8e-4 \\\\\n",
      "\\rowcolor{gray!10}1420 & 2048 & 5504 & 256 & 512 & 4 & 24 & 256 & 8e-4 \\\\\n",
      "1420 & 2048 & 5504 & 256 & 512 & 4 & 24 & 256 & 7e-4 \\\\\n",
      "\\rowcolor{gray!10}2780 & 2560 & 6848 & 256 & 512 & 5 & 32 & 512 & 7e-4 \\\\\n",
      "6865 & 4096 & 10944 & 256 & 512 & 8 & 32 & 512 & 5e-4 \\\\\n",
      "\\rowcolor{gray!10}6865 & 4096 & 10944 & 256 & 512 & 8 & 32 & 256 & 5e-4 \\\\\n",
      "6865 & 4096 & 10944 & 256 & 512 & 8 & 32 & 512 & 4e-4 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mlstm token param table\n",
    "df = get_experiment_set_df(\"tokenparam_ctx8192\", \"mlstm\")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for xLSTM models trained with the Token/Param configuration.\",\n",
    "    label=\"tab:tokenparam_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2056bef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not extract chunk size from backend kwargs '{'backend_name': 'chunkwise--triton_xl_chunk'}' from run WandBRunData(name=dclm_mLSTMv1_160M_ctx8192_lr0.003_steps18000_gbs128, id=djkqz1dc, path=['xlstm', 'xlstm_jax', 'djkqz1dc'], created_at=2024-12-19T15:53:36Z).Using default chunk size of 64.\n",
      "Could not extract chunk size from backend kwargs '{'backend_name': 'max_triton_noslice'}' from run WandBRunData(name=dclm_mLSTMv1_7B_ctx8192_gbs512, id=hphy3hsq, path=['xlstm', 'xlstm_jax', 'hphy3hsq'], created_at=2024-11-06T22:21:03Z).Using default chunk size of 64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for Transformer models trained with the Token/Param configuration.}\n",
      "\\label{tab:tokenparam_hyperparams}\n",
      "\\begin{tabular}{r|rrrrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{v}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ & $B$ (batch) & LR \\\\\n",
      "\\midrule\n",
      "162 & 768 & 2048 & 64 & 12 & 12 & 128 & 3e-3 \\\\\n",
      "\\rowcolor{gray!10}162 & 768 & 2048 & 64 & 12 & 12 & 128 & 1e-3 \\\\\n",
      "406 & 1024 & 2752 & 64 & 16 & 24 & 128 & 3e-3 \\\\\n",
      "\\rowcolor{gray!10}406 & 1024 & 2752 & 64 & 16 & 24 & 128 & 1e-3 \\\\\n",
      "834 & 1536 & 4096 & 96 & 16 & 24 & 256 & 1e-3 \\\\\n",
      "\\rowcolor{gray!10}1420 & 2048 & 5504 & 128 & 16 & 24 & 256 & 8e-4 \\\\\n",
      "2779 & 2560 & 6848 & 80 & 32 & 32 & 512 & 7e-4 \\\\\n",
      "\\rowcolor{gray!10}6863 & 4096 & 10944 & 128 & 32 & 32 & 256 & 5e-4 \\\\\n",
      "6863 & 4096 & 10944 & 128 & 32 & 32 & 512 & 5e-4 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama token param table\n",
    "df = get_experiment_set_df(\"tokenparam_ctx8192\", \"llama\")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for Transformer models trained with the Token/Param configuration.\",\n",
    "    label=\"tab:tokenparam_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0464e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for xLSTM models trained with the IsoFLOP configuration.}\n",
      "\\label{tab:xlstm_isoflop_hyperparams}\n",
      "\\begin{adjustbox}{max height=0.5\\textheight,center}\n",
      "\\begin{tabular}{r|rrrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{qk}}$ & $d_{\\text{hv}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ \\\\\n",
      "\\midrule\n",
      "83 & 512 & 1408 & 64 & 128 & 4 & 10 \\\\\n",
      "\\rowcolor{gray!10}90 & 512 & 1408 & 64 & 128 & 4 & 12 \\\\\n",
      "96 & 512 & 1408 & 64 & 128 & 4 & 14 \\\\\n",
      "\\rowcolor{gray!10}102 & 512 & 1408 & 64 & 128 & 4 & 16 \\\\\n",
      "114 & 640 & 1728 & 64 & 128 & 5 & 10 \\\\\n",
      "\\rowcolor{gray!10}123 & 640 & 1728 & 64 & 128 & 5 & 12 \\\\\n",
      "128 & 640 & 1728 & 64 & 128 & 5 & 13 \\\\\n",
      "\\rowcolor{gray!10}133 & 640 & 1728 & 64 & 128 & 5 & 14 \\\\\n",
      "143 & 640 & 1728 & 64 & 128 & 5 & 16 \\\\\n",
      "\\rowcolor{gray!10}164 & 768 & 2112 & 64 & 128 & 6 & 12 \\\\\n",
      "185 & 768 & 2112 & 64 & 128 & 6 & 15 \\\\\n",
      "\\rowcolor{gray!10}207 & 896 & 2432 & 64 & 128 & 7 & 12 \\\\\n",
      "207 & 768 & 2112 & 64 & 128 & 6 & 18 \\\\\n",
      "\\rowcolor{gray!10}236 & 896 & 2432 & 64 & 128 & 7 & 15 \\\\\n",
      "265 & 896 & 2432 & 64 & 128 & 7 & 18 \\\\\n",
      "\\rowcolor{gray!10}295 & 896 & 2432 & 64 & 128 & 7 & 21 \\\\\n",
      "324 & 896 & 2432 & 64 & 128 & 7 & 24 \\\\\n",
      "\\rowcolor{gray!10}330 & 1024 & 2752 & 128 & 256 & 4 & 18 \\\\\n",
      "353 & 896 & 2432 & 64 & 128 & 7 & 27 \\\\\n",
      "\\rowcolor{gray!10}368 & 1024 & 2752 & 128 & 256 & 4 & 21 \\\\\n",
      "406 & 1024 & 2752 & 128 & 256 & 4 & 24 \\\\\n",
      "\\rowcolor{gray!10}444 & 1024 & 2752 & 128 & 256 & 4 & 27 \\\\\n",
      "482 & 1024 & 2752 & 128 & 256 & 4 & 30 \\\\\n",
      "\\rowcolor{gray!10}503 & 1152 & 3136 & 64 & 128 & 9 & 24 \\\\\n",
      "552 & 1152 & 3136 & 64 & 128 & 9 & 27 \\\\\n",
      "\\rowcolor{gray!10}601 & 1152 & 3136 & 64 & 128 & 9 & 30 \\\\\n",
      "604 & 1280 & 3456 & 128 & 256 & 5 & 24 \\\\\n",
      "\\rowcolor{gray!10}664 & 1280 & 3456 & 128 & 256 & 5 & 27 \\\\\n",
      "715 & 1408 & 3776 & 64 & 128 & 11 & 24 \\\\\n",
      "\\rowcolor{gray!10}724 & 1280 & 3456 & 128 & 256 & 5 & 30 \\\\\n",
      "787 & 1408 & 3776 & 64 & 128 & 11 & 27 \\\\\n",
      "\\rowcolor{gray!10}841 & 1536 & 4160 & 128 & 256 & 6 & 24 \\\\\n",
      "859 & 1408 & 3776 & 64 & 128 & 11 & 30 \\\\\n",
      "\\rowcolor{gray!10}927 & 1536 & 4160 & 128 & 256 & 6 & 27 \\\\\n",
      "1013 & 1536 & 4160 & 128 & 256 & 6 & 30 \\\\\n",
      "\\rowcolor{gray!10}1108 & 1792 & 4800 & 128 & 256 & 7 & 24 \\\\\n",
      "1224 & 1792 & 4800 & 128 & 256 & 7 & 27 \\\\\n",
      "\\rowcolor{gray!10}1340 & 1792 & 4800 & 128 & 256 & 7 & 30 \\\\\n",
      "1421 & 2048 & 5504 & 128 & 256 & 8 & 24 \\\\\n",
      "\\rowcolor{gray!10}1573 & 2048 & 5504 & 128 & 256 & 8 & 27 \\\\\n",
      "1772 & 2304 & 6208 & 128 & 256 & 9 & 24 \\\\\n",
      "\\rowcolor{gray!10}1876 & 2048 & 5504 & 128 & 256 & 8 & 33 \\\\\n",
      "1964 & 2304 & 6208 & 128 & 256 & 9 & 27 \\\\\n",
      "\\rowcolor{gray!10}2028 & 2048 & 5504 & 128 & 256 & 8 & 36 \\\\\n",
      "2157 & 2304 & 6208 & 128 & 256 & 9 & 30 \\\\\n",
      "\\rowcolor{gray!10}2350 & 2304 & 6208 & 128 & 256 & 9 & 33 \\\\\n",
      "2781 & 2560 & 6848 & 128 & 256 & 10 & 32 \\\\\n",
      "\\rowcolor{gray!10}3017 & 2560 & 6848 & 128 & 256 & 10 & 35 \\\\\n",
      "3150 & 2816 & 7552 & 128 & 256 & 11 & 30 \\\\\n",
      "\\rowcolor{gray!10}3254 & 2560 & 6848 & 128 & 256 & 10 & 38 \\\\\n",
      "3342 & 2816 & 7552 & 128 & 256 & 11 & 32 \\\\\n",
      "\\rowcolor{gray!10}3533 & 2816 & 7552 & 128 & 256 & 11 & 34 \\\\\n",
      "3724 & 2816 & 7552 & 128 & 256 & 11 & 36 \\\\\n",
      "\\rowcolor{gray!10}3726 & 3072 & 8256 & 128 & 256 & 12 & 30 \\\\\n",
      "3954 & 3072 & 8256 & 128 & 256 & 12 & 32 \\\\\n",
      "\\rowcolor{gray!10}4410 & 3072 & 8256 & 128 & 256 & 12 & 36 \\\\\n",
      "4597 & 3328 & 8896 & 128 & 256 & 13 & 32 \\\\\n",
      "\\rowcolor{gray!10}5130 & 3328 & 8896 & 128 & 256 & 13 & 36 \\\\\n",
      "5311 & 3584 & 9600 & 128 & 256 & 14 & 32 \\\\\n",
      "\\rowcolor{gray!10}5930 & 3584 & 9600 & 128 & 256 & 14 & 36 \\\\\n",
      "6464 & 4096 & 10944 & 128 & 256 & 16 & 30 \\\\\n",
      "\\rowcolor{gray!10}6867 & 4096 & 10944 & 128 & 256 & 16 & 32 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mlstm isoflop table\n",
    "df = get_experiment_set_df(\n",
    "    [\"isoflop_ctx2048\", \"isoflop_ctx8192\", \"isoflop_ctx16384\"], \"mlstm\"\n",
    ")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for xLSTM models trained with the IsoFLOP configuration.\",\n",
    "    label=\"tab:xlstm_isoflop_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "colored_latex_table = add_adjustbox_scaling(colored_latex_table, height_scale=0.5)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d40b3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{List of hyperparameters for Transformer models trained with the IsoFLOP configuration.}\n",
      "\\label{tab:transformer_isoflop_hyperparams}\n",
      "\\begin{adjustbox}{max height=0.5\\textheight,center}\n",
      "\\begin{tabular}{r|rrrrr}\n",
      "\\toprule\n",
      "\\#Params (M) & $d_{\\text{model}}$ & $d_{\\text{ff}}$ & $d_{\\text{v}}$ & $n_{\\text{heads}}$ & $n_{\\text{layer}}$ \\\\\n",
      "\\midrule\n",
      "83 & 512 & 1408 & 64 & 8 & 10 \\\\\n",
      "\\rowcolor{gray!10}90 & 512 & 1408 & 64 & 8 & 12 \\\\\n",
      "96 & 512 & 1408 & 64 & 8 & 14 \\\\\n",
      "\\rowcolor{gray!10}102 & 512 & 1408 & 64 & 8 & 16 \\\\\n",
      "113 & 640 & 1728 & 64 & 10 & 10 \\\\\n",
      "\\rowcolor{gray!10}128 & 640 & 1728 & 64 & 10 & 13 \\\\\n",
      "133 & 640 & 1728 & 64 & 10 & 14 \\\\\n",
      "\\rowcolor{gray!10}143 & 640 & 1728 & 64 & 10 & 16 \\\\\n",
      "162 & 768 & 2048 & 64 & 12 & 12 \\\\\n",
      "\\rowcolor{gray!10}183 & 768 & 2048 & 64 & 12 & 15 \\\\\n",
      "204 & 768 & 2048 & 64 & 12 & 18 \\\\\n",
      "\\rowcolor{gray!10}207 & 896 & 2432 & 64 & 14 & 12 \\\\\n",
      "236 & 896 & 2432 & 64 & 14 & 15 \\\\\n",
      "\\rowcolor{gray!10}265 & 896 & 2432 & 64 & 14 & 18 \\\\\n",
      "294 & 896 & 2432 & 64 & 14 & 21 \\\\\n",
      "\\rowcolor{gray!10}324 & 896 & 2432 & 64 & 14 & 24 \\\\\n",
      "330 & 1024 & 2752 & 64 & 16 & 18 \\\\\n",
      "\\rowcolor{gray!10}368 & 1024 & 2752 & 64 & 16 & 21 \\\\\n",
      "406 & 1024 & 2752 & 64 & 16 & 24 \\\\\n",
      "\\rowcolor{gray!10}444 & 1024 & 2752 & 64 & 16 & 27 \\\\\n",
      "482 & 1024 & 2752 & 64 & 16 & 30 \\\\\n",
      "\\rowcolor{gray!10}498 & 1152 & 3072 & 128 & 9 & 24 \\\\\n",
      "545 & 1152 & 3072 & 128 & 9 & 27 \\\\\n",
      "\\rowcolor{gray!10}593 & 1152 & 3072 & 128 & 9 & 30 \\\\\n",
      "604 & 1280 & 3456 & 128 & 10 & 24 \\\\\n",
      "\\rowcolor{gray!10}664 & 1280 & 3456 & 128 & 10 & 27 \\\\\n",
      "714 & 1408 & 3776 & 128 & 11 & 24 \\\\\n",
      "\\rowcolor{gray!10}723 & 1280 & 3456 & 128 & 10 & 30 \\\\\n",
      "786 & 1408 & 3776 & 128 & 11 & 27 \\\\\n",
      "\\rowcolor{gray!10}834 & 1536 & 4096 & 128 & 12 & 24 \\\\\n",
      "858 & 1408 & 3776 & 128 & 11 & 30 \\\\\n",
      "\\rowcolor{gray!10}919 & 1536 & 4096 & 128 & 12 & 27 \\\\\n",
      "1003 & 1536 & 4096 & 128 & 12 & 30 \\\\\n",
      "\\rowcolor{gray!10}1107 & 1792 & 4800 & 128 & 14 & 24 \\\\\n",
      "1223 & 1792 & 4800 & 128 & 14 & 27 \\\\\n",
      "\\rowcolor{gray!10}1339 & 1792 & 4800 & 128 & 14 & 30 \\\\\n",
      "1420 & 2048 & 5504 & 128 & 16 & 24 \\\\\n",
      "\\rowcolor{gray!10}1572 & 2048 & 5504 & 128 & 16 & 27 \\\\\n",
      "1723 & 2048 & 5504 & 128 & 16 & 30 \\\\\n",
      "\\rowcolor{gray!10}1760 & 2304 & 6144 & 128 & 18 & 24 \\\\\n",
      "1951 & 2304 & 6144 & 128 & 18 & 27 \\\\\n",
      "\\rowcolor{gray!10}2142 & 2304 & 6144 & 128 & 18 & 30 \\\\\n",
      "2334 & 2304 & 6144 & 128 & 18 & 33 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llama isoflop table\n",
    "df = get_experiment_set_df(\n",
    "    [\"isoflop_ctx2048\", \"isoflop_ctx8192\", \"isoflop_ctx16384\"], \"llama\"\n",
    ")\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={\"LR\": lambda x: f\"{x:.0e}\".replace(\"e-0\", \"e-\").replace(\"e+0\", \"e+\")},\n",
    "    caption=\"List of hyperparameters for Transformer models trained with the IsoFLOP configuration.\",\n",
    "    label=\"tab:transformer_isoflop_hyperparams\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "colored_latex_table = add_adjustbox_scaling(colored_latex_table, height_scale=0.5)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f6cb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "experiment_set_ctx_length",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "run_tag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "global_batch_size",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "num_params",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ffn_dim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "head_dim_qk",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "head_dim_v",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_heads",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "num_blocks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "context_length",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "train/.loss_mean",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "98723de4-fc86-47e6-92bd-6ea90ed8abb9",
       "rows": [
        [
         "257",
         "isoflop_ctx8192",
         "dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb32_ed4096_nh16_pf2.667_gbs256",
         "sclaw_iso_round8",
         "0.0009",
         "256.0",
         "6867522560.0",
         "10944",
         "128.0",
         "256",
         "16",
         "32",
         "8192",
         "2.562390444479373"
        ],
        [
         "258",
         "isoflop_ctx8192",
         "dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb30_ed4096_nh16_pf2.667_gbs256",
         "sclaw_iso_round8",
         "0.0009",
         "256.0",
         "6464058304.0",
         "10944",
         "128.0",
         "256",
         "16",
         "30",
         "8192",
         "2.553666337935594"
        ],
        [
         "637",
         "tokenparam_ctx8192",
         "dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps73000_gbs512",
         "scl_mlstm_7B",
         "0.0005",
         "512.0",
         "6865424896.0",
         "10944",
         "256.0",
         "512",
         "8",
         "32",
         "8192",
         "2.206035895198121"
        ],
        [
         "638",
         "tokenparam_ctx8192",
         "dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps76000_gbs256",
         "scl_mlstm_7B",
         "0.0005",
         "256.0",
         "6865424896.0",
         "10944",
         "256.0",
         "512",
         "8",
         "32",
         "8192",
         "2.25183173169402"
        ],
        [
         "639",
         "tokenparam_ctx8192",
         "dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps181000_gbs512",
         "scl_mlstm_7B",
         "0.0005",
         "512.0",
         "6865424896.0",
         "10944",
         "256.0",
         "512",
         "8",
         "32",
         "8192",
         "2.1502073137546587"
        ],
        [
         "640",
         "tokenparam_ctx8192",
         "dclm_mLSTMv1_7B_ctx8192_gbs512",
         "dclm_mLSTMv1_7B_longrun_pretraining_final",
         "0.0004",
         "512.0",
         "6865424896.0",
         "10944",
         "256.0",
         "512",
         "8",
         "32",
         "8192",
         "2.1004479801719813"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_set_ctx_length</th>\n",
       "      <th>name</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>num_params</th>\n",
       "      <th>ffn_dim</th>\n",
       "      <th>head_dim_qk</th>\n",
       "      <th>head_dim_v</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>context_length</th>\n",
       "      <th>train/.loss_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>isoflop_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb3...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.867523e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>128.0</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.562390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>isoflop_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb3...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.464058e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>128.0</td>\n",
       "      <td>256</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.553666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps73000_gb...</td>\n",
       "      <td>scl_mlstm_7B</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.206036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps76000_gb...</td>\n",
       "      <td>scl_mlstm_7B</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.251832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps181000_g...</td>\n",
       "      <td>scl_mlstm_7B</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.150207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>tokenparam_ctx8192</td>\n",
       "      <td>dclm_mLSTMv1_7B_ctx8192_gbs512</td>\n",
       "      <td>dclm_mLSTMv1_7B_longrun_pretraining_final</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>512.0</td>\n",
       "      <td>6.865425e+09</td>\n",
       "      <td>10944</td>\n",
       "      <td>256.0</td>\n",
       "      <td>512</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>8192</td>\n",
       "      <td>2.100448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    experiment_set_ctx_length  \\\n",
       "257           isoflop_ctx8192   \n",
       "258           isoflop_ctx8192   \n",
       "637        tokenparam_ctx8192   \n",
       "638        tokenparam_ctx8192   \n",
       "639        tokenparam_ctx8192   \n",
       "640        tokenparam_ctx8192   \n",
       "\n",
       "                                                  name  \\\n",
       "257  dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7200_nb3...   \n",
       "258  dclm_mLSTMv1_7B_ctx8192_lr0.0009_steps7600_nb3...   \n",
       "637  dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps73000_gb...   \n",
       "638  dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps76000_gb...   \n",
       "639  dclm_mLSTMv1_7B_ctx8192_lr0.0005_steps181000_g...   \n",
       "640                     dclm_mLSTMv1_7B_ctx8192_gbs512   \n",
       "\n",
       "                                       run_tag  learning_rate  \\\n",
       "257                           sclaw_iso_round8         0.0009   \n",
       "258                           sclaw_iso_round8         0.0009   \n",
       "637                               scl_mlstm_7B         0.0005   \n",
       "638                               scl_mlstm_7B         0.0005   \n",
       "639                               scl_mlstm_7B         0.0005   \n",
       "640  dclm_mLSTMv1_7B_longrun_pretraining_final         0.0004   \n",
       "\n",
       "     global_batch_size    num_params  ffn_dim  head_dim_qk  head_dim_v  \\\n",
       "257              256.0  6.867523e+09    10944        128.0         256   \n",
       "258              256.0  6.464058e+09    10944        128.0         256   \n",
       "637              512.0  6.865425e+09    10944        256.0         512   \n",
       "638              256.0  6.865425e+09    10944        256.0         512   \n",
       "639              512.0  6.865425e+09    10944        256.0         512   \n",
       "640              512.0  6.865425e+09    10944        256.0         512   \n",
       "\n",
       "     num_heads  num_blocks  context_length  train/.loss_mean  \n",
       "257         16          32            8192          2.562390  \n",
       "258         16          30            8192          2.553666  \n",
       "637          8          32            8192          2.206036  \n",
       "638          8          32            8192          2.251832  \n",
       "639          8          32            8192          2.150207  \n",
       "640          8          32            8192          2.100448  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlstm_df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"mlstm\")\n",
    "mlstm_df[mlstm_df[\"embedding_dim\"] == 4096][\n",
    "    [\n",
    "        \"experiment_set_ctx_length\",\n",
    "        \"name\",\n",
    "        \"run_tag\",\n",
    "        \"learning_rate\",\n",
    "        \"global_batch_size\",\n",
    "        \"num_params\",\n",
    "        \"ffn_dim\",\n",
    "        \"head_dim_qk\",\n",
    "        \"head_dim_v\",\n",
    "        \"num_heads\",\n",
    "        \"num_blocks\",\n",
    "        \"context_length\",\n",
    "        \"train/.loss_mean\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4891ba5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_204524/1683853249.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df[df[\"experiment_set_ctx_length\"] == \"isoflop_ctx8192\"][\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "run_tag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "global_batch_size",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "IsoFLOP",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "learning_rate",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "08773825-90fe-4e32-b952-29189fba52e3",
       "rows": [
        [
         "90",
         "dclm_mLSTMv1_100M_ctx8192_lr0.003_steps192000_nb10_ed640_nh5_pf2.667_gbs128",
         "nb10_ed640_nh5_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "94",
         "dclm_mLSTMv1_100M_ctx8192_lr0.003_steps162000_nb13_ed640_nh5_pf2.667_gbs128",
         "nb13_ed640_nh5_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "98",
         "dclm_mLSTMv1_100M_ctx8192_lr0.003_steps140000_nb16_ed640_nh5_pf2.667_gbs128",
         "nb16_ed640_nh5_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "102",
         "dclm_mLSTMv1_160M_ctx8192_lr0.003_steps126500_nb12_ed768_nh6_pf2.667_gbs128",
         "nb12_ed768_nh6_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "109",
         "dclm_mLSTMv1_160M_ctx8192_lr0.003_steps94000_nb18_ed768_nh6_pf2.667_gbs128",
         "nb18_ed768_nh6_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "113",
         "dclm_mLSTMv1_400M_ctx8192_lr0.003_steps44000_nb24_ed1024_nh4_pf2.667_gbs128",
         "nb24_ed1024_nh4_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "117",
         "dclm_mLSTMv1_400M_ctx8192_lr0.003_steps39500_nb27_ed1024_nh4_pf2.667_gbs128",
         "nb27_ed1024_nh4_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "121",
         "dclm_mLSTMv1_400M_ctx8192_lr0.003_steps36000_nb30_ed1024_nh4_pf2.667_gbs128",
         "nb30_ed1024_nh4_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "124",
         "dclm_mLSTMv1_600M_ctx8192_lr0.002_steps29000_nb24_ed1280_nh5_pf2.667_gbs128",
         "nb24_ed1280_nh5_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "127",
         "dclm_mLSTMv1_600M_ctx8192_lr0.002_steps26000_nb27_ed1280_nh5_pf2.667_gbs128",
         "nb27_ed1280_nh5_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "130",
         "dclm_mLSTMv1_600M_ctx8192_lr0.002_steps24000_nb30_ed1280_nh5_pf2.667_gbs128",
         "nb30_ed1280_nh5_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "132",
         "dclm_mLSTMv1_830M_ctx8192_lr0.002_steps20600_nb24_ed1536_nh6_pf2.667_gbs128",
         "nb24_ed1536_nh6_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "135",
         "dclm_mLSTMv1_830M_ctx8192_lr0.002_steps18600_nb27_ed1536_nh6_pf2.667_gbs128",
         "nb27_ed1536_nh6_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "138",
         "dclm_mLSTMv1_830M_ctx8192_lr0.002_steps16800_nb30_ed1536_nh6_pf2.667_gbs128",
         "nb30_ed1536_nh6_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "141",
         "dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps15400_nb24_ed1792_nh7_pf2.667_gbs128",
         "nb24_ed1792_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "144",
         "dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps13800_nb27_ed1792_nh7_pf2.667_gbs128",
         "nb27_ed1792_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "147",
         "dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps12600_nb30_ed1792_nh7_pf2.667_gbs128",
         "nb30_ed1792_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "150",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps12000_nb24_ed2048_nh8_pf2.667_gbs128",
         "nb24_ed2048_nh8_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "153",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps10800_nb27_ed2048_nh8_pf2.667_gbs128",
         "nb27_ed2048_nh8_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "155",
         "dclm_mLSTMv1_2.7B_ctx8192_lr0.002_steps6000_nb32_ed2560_nh10_pf2.667_gbs128",
         "nb32_ed2560_nh10_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "157",
         "dclm_mLSTMv1_2.7B_ctx8192_lr0.002_steps5400_nb35_ed2560_nh10_pf2.667_gbs128",
         "nb35_ed2560_nh10_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "158",
         "dclm_mLSTMv1_2.7B_ctx8192_lr0.002_steps5000_nb38_ed2560_nh10_pf2.667_gbs128",
         "nb38_ed2560_nh10_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "163",
         "dclm_mLSTMv1_200M_ctx8192_lr0.003_steps97500_nb12_ed896_nh7_pf2.667_gbs128",
         "nb12_ed896_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "166",
         "dclm_mLSTMv1_200M_ctx8192_lr0.003_steps82500_nb15_ed896_nh7_pf2.667_gbs128",
         "nb15_ed896_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "170",
         "dclm_mLSTMv1_200M_ctx8192_lr0.003_steps71500_nb18_ed896_nh7_pf2.667_gbs128",
         "nb18_ed896_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "173",
         "dclm_mLSTMv1_200M_ctx8192_lr0.003_steps63000_nb21_ed896_nh7_pf2.667_gbs128",
         "nb21_ed896_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "179",
         "dclm_mLSTMv1_500M_ctx8192_lr0.002_steps35500_nb24_ed1152_nh9_pf2.667_gbs128",
         "nb24_ed1152_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "182",
         "dclm_mLSTMv1_500M_ctx8192_lr0.002_steps32000_nb27_ed1152_nh9_pf2.667_gbs128",
         "nb27_ed1152_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "185",
         "dclm_mLSTMv1_500M_ctx8192_lr0.002_steps29250_nb30_ed1152_nh9_pf2.667_gbs128",
         "nb30_ed1152_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "187",
         "dclm_mLSTMv1_700M_ctx8192_lr0.002_steps24500_nb24_ed1408_nh11_pf2.667_gbs128",
         "nb24_ed1408_nh11_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "189",
         "dclm_mLSTMv1_700M_ctx8192_lr0.002_steps22000_nb27_ed1408_nh11_pf2.667_gbs128",
         "nb27_ed1408_nh11_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "191",
         "dclm_mLSTMv1_700M_ctx8192_lr0.002_steps20000_nb30_ed1408_nh11_pf2.667_gbs128",
         "nb30_ed1408_nh11_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "195",
         "dclm_mLSTMv1_200M_ctx8192_lr0.003_steps51000_nb27_ed896_nh7_pf2.667_gbs128",
         "nb27_ed896_nh7_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "199",
         "dclm_mLSTMv1_400M_ctx8192_lr0.003_steps49000_nb21_ed1024_nh4_pf2.667_gbs128",
         "nb21_ed1024_nh4_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "203",
         "dclm_mLSTMv1_400M_ctx8192_lr0.003_steps55750_nb18_ed1024_nh4_pf2.667_gbs128",
         "nb18_ed1024_nh4_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.003"
        ],
        [
         "205",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps8800_nb33_ed2048_nh8_pf2.667_gbs128",
         "nb33_ed2048_nh8_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "207",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps8200_nb36_ed2048_nh8_pf2.667_gbs128",
         "nb36_ed2048_nh8_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "210",
         "dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps9600_nb24_ed2304_nh9_pf2.667_gbs128",
         "nb24_ed2304_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "212",
         "dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps8600_nb27_ed2304_nh9_pf2.667_gbs128",
         "nb27_ed2304_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "214",
         "dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps7800_nb30_ed2304_nh9_pf2.667_gbs128",
         "nb30_ed2304_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "215",
         "dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps7000_nb33_ed2304_nh9_pf2.667_gbs128",
         "nb33_ed2304_nh9_pf2.667,sclaw_iso",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "227",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps8800_nb33_ed2048_nh8_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "228",
         "dclm_mLSTMv1_1.8B_ctx8192_lr0.002_steps9600_nb24_ed2304_nh9_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "229",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps10800_nb27_ed2048_nh8_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "230",
         "dclm_mLSTMv1_1.4B_ctx8192_lr0.002_steps12000_nb24_ed2048_nh8_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "231",
         "dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps12600_nb30_ed1792_nh7_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "232",
         "dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps13800_nb27_ed1792_nh7_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "233",
         "dclm_mLSTMv1_1.1B_ctx8192_lr0.002_steps15400_nb24_ed1792_nh7_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "234",
         "dclm_mLSTMv1_830M_ctx8192_lr0.002_steps16800_nb30_ed1536_nh6_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ],
        [
         "235",
         "dclm_mLSTMv1_830M_ctx8192_lr0.002_steps18600_nb27_ed1536_nh6_pf2.667_gbs128",
         "sclaw_iso_round5",
         "128.0",
         "1e+20",
         "0.002"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 154
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>run_tag</th>\n",
       "      <th>global_batch_size</th>\n",
       "      <th>IsoFLOP</th>\n",
       "      <th>learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>dclm_mLSTMv1_100M_ctx8192_lr0.003_steps192000_...</td>\n",
       "      <td>nb10_ed640_nh5_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>dclm_mLSTMv1_100M_ctx8192_lr0.003_steps162000_...</td>\n",
       "      <td>nb13_ed640_nh5_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>dclm_mLSTMv1_100M_ctx8192_lr0.003_steps140000_...</td>\n",
       "      <td>nb16_ed640_nh5_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>dclm_mLSTMv1_160M_ctx8192_lr0.003_steps126500_...</td>\n",
       "      <td>nb12_ed768_nh6_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>dclm_mLSTMv1_160M_ctx8192_lr0.003_steps94000_n...</td>\n",
       "      <td>nb18_ed768_nh6_pf2.667,sclaw_iso</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1e+20</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps8200_n...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_n...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>dclm_mLSTMv1_4.5B_ctx8192_lr0.0009_steps9600_n...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>dclm_mLSTMv1_4.5B_ctx8192_lr0.0009_steps10600_...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>dclm_mLSTMv1_3.7B_ctx8192_lr0.0009_steps11200_...</td>\n",
       "      <td>sclaw_iso_round8</td>\n",
       "      <td>256.0</td>\n",
       "      <td>6e+20</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name  \\\n",
       "90   dclm_mLSTMv1_100M_ctx8192_lr0.003_steps192000_...   \n",
       "94   dclm_mLSTMv1_100M_ctx8192_lr0.003_steps162000_...   \n",
       "98   dclm_mLSTMv1_100M_ctx8192_lr0.003_steps140000_...   \n",
       "102  dclm_mLSTMv1_160M_ctx8192_lr0.003_steps126500_...   \n",
       "109  dclm_mLSTMv1_160M_ctx8192_lr0.003_steps94000_n...   \n",
       "..                                                 ...   \n",
       "259  dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps8200_n...   \n",
       "260  dclm_mLSTMv1_5.5B_ctx8192_lr0.0009_steps9200_n...   \n",
       "261  dclm_mLSTMv1_4.5B_ctx8192_lr0.0009_steps9600_n...   \n",
       "262  dclm_mLSTMv1_4.5B_ctx8192_lr0.0009_steps10600_...   \n",
       "263  dclm_mLSTMv1_3.7B_ctx8192_lr0.0009_steps11200_...   \n",
       "\n",
       "                              run_tag  global_batch_size IsoFLOP  \\\n",
       "90   nb10_ed640_nh5_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "94   nb13_ed640_nh5_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "98   nb16_ed640_nh5_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "102  nb12_ed768_nh6_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "109  nb18_ed768_nh6_pf2.667,sclaw_iso              128.0   1e+20   \n",
       "..                                ...                ...     ...   \n",
       "259                  sclaw_iso_round8              256.0   6e+20   \n",
       "260                  sclaw_iso_round8              256.0   6e+20   \n",
       "261                  sclaw_iso_round8              256.0   6e+20   \n",
       "262                  sclaw_iso_round8              256.0   6e+20   \n",
       "263                  sclaw_iso_round8              256.0   6e+20   \n",
       "\n",
       "     learning_rate  \n",
       "90          0.0030  \n",
       "94          0.0030  \n",
       "98          0.0030  \n",
       "102         0.0030  \n",
       "109         0.0030  \n",
       "..             ...  \n",
       "259         0.0009  \n",
       "260         0.0009  \n",
       "261         0.0009  \n",
       "262         0.0009  \n",
       "263         0.0009  \n",
       "\n",
       "[154 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "df[df[\"experiment_set_ctx_length\"] == \"isoflop_ctx8192\"][\n",
    "    df[\"IsoFLOP\"].isin([\"6e+18\", \"6e+20\", \"1e+20\"])\n",
    "][\n",
    "    [\n",
    "        \"name\",\n",
    "        \"run_tag\",\n",
    "        \"global_batch_size\",\n",
    "        \"IsoFLOP\",\n",
    "        \"learning_rate\",\n",
    "    ]\n",
    "].sort_values(by=[\"IsoFLOP\", \"global_batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f333f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isoflop_batch_size_df(ctx_length=None) -> pd.DataFrame:\n",
    "    df = get_all_parametric_sclaw_fit_data_dataframe(model_type=\"all\")\n",
    "    batch_size_df = (\n",
    "        df[[\"IsoFLOP\", \"context_length\", \"global_batch_size\"]]\n",
    "        .drop_duplicates()\n",
    "        .sort_values(by=[\"context_length\", \"IsoFLOP\", \"global_batch_size\"])\n",
    "        .dropna()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    batch_size_df.rename(columns={\"global_batch_size\": \"bs_in_seqs\"}, inplace=True)\n",
    "    batch_size_df[\"bs_in_seqs\"] = batch_size_df[\"bs_in_seqs\"].astype(int)\n",
    "\n",
    "    batch_size_df[\"bs_in_tokens\"] = (\n",
    "        batch_size_df[\"bs_in_seqs\"] * batch_size_df[\"context_length\"]\n",
    "    )\n",
    "    batch_size_df[\"bs_in_tokens\"] = batch_size_df[\"bs_in_tokens\"].astype(int)\n",
    "\n",
    "    # rename columns for latex\n",
    "    col_name_map = {\n",
    "        \"IsoFLOP\": \"IsoFLOP\",\n",
    "        \"context_length\": r\"$T$ (ctx)\",\n",
    "        \"bs_in_seqs\": r\"$B$ (seqs)\",\n",
    "        \"bs_in_tokens\": r\"$B \\times T$ (tokens)\",\n",
    "    }\n",
    "    batch_size_df = batch_size_df.rename(columns=col_name_map)\n",
    "    if ctx_length is not None:\n",
    "        return batch_size_df[batch_size_df[r\"$T$ (ctx)\"] == ctx_length]\n",
    "    return batch_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "409fe5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Batch sizes used for models trained with the IsoFLOP configuration at context length 8192.}\n",
      "\\label{tab:isoflop_batch_sizes}\n",
      "\\begin{tabular}{r|rrr}\n",
      "\\toprule\n",
      "IsoFLOP & $T$ (ctx) & $B$ (seqs) & $B \\times T$ (tokens) \\\\\n",
      "\\midrule\n",
      "\\rowcolor{gray!10}1e+19 & 8192 & 128 & 1,048,576 \\\\\n",
      "1e+20 & 8192 & 128 & 1,048,576 \\\\\n",
      "\\rowcolor{gray!10}3e+19 & 8192 & 128 & 1,048,576 \\\\\n",
      "6e+18 & 8192 & 128 & 1,048,576 \\\\\n",
      "\\rowcolor{gray!10}6e+20 & 8192 & 256 & 2,097,152 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = get_isoflop_batch_size_df(8192)\n",
    "latex_table = df.to_latex(\n",
    "    index=False,\n",
    "    formatters={r\"$B \\times T$ (tokens)\": lambda x: f\"{x:,}\"},\n",
    "    caption=\"Batch sizes used for models trained with the IsoFLOP configuration at context length 8192.\",\n",
    "    label=\"tab:isoflop_batch_sizes\",\n",
    "    longtable=False,\n",
    "    column_format=\"r|\" + \"r\" * (len(df.columns) - 1),\n",
    ")\n",
    "colored_latex_table = add_row_colors(latex_table)\n",
    "# colored_latex_table = add_adjustbox_scaling(colored_latex_table, height_scale=0.5)\n",
    "print(colored_latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ecf742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533039c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_260210_pt291cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
